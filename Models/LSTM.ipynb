{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project')\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "%matplotlib widget\n",
    "import pandas as pd # for data manipulation\n",
    "from Models.load_data import *\n",
    "from Models.helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "\n",
    "path = r'C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project\\Training_data'\n",
    "features_list = os.listdir(os.path.join(path, r'features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping JETno97612.csv: sequence length 6001 is unexpected.\n",
      "Skipping JETno87125.csv: sequence length 6001 is unexpected.\n",
      "Skipping JETno91071.csv: sequence length 6001 is unexpected.\n",
      "Skipping JETno99471.csv: sequence length 6001 is unexpected.\n",
      "Processed 296 sequences with shape (60, 600).\n",
      "Global feature min (train set only): [ 0.0000000e+00 -3.2111890e+06 -9.1752766e+13  0.0000000e+00\n",
      " -8.4846081e+13  0.0000000e+00  0.0000000e+00 -3.2102978e+06\n",
      " -9.2097940e+13  0.0000000e+00 -7.3527542e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.2099495e+06 -9.2103753e+13  0.0000000e+00\n",
      " -7.2064157e+13  0.0000000e+00  0.0000000e+00 -3.2106140e+06\n",
      " -9.1307901e+13  0.0000000e+00 -9.4136540e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.2105688e+06 -9.0980284e+13  0.0000000e+00\n",
      " -8.5559138e+13  0.0000000e+00  0.0000000e+00 -3.2106132e+06\n",
      " -9.0181126e+13  0.0000000e+00 -9.5513295e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.2092092e+06 -8.9826750e+13  0.0000000e+00\n",
      " -7.5441352e+13  0.0000000e+00  0.0000000e+00 -3.2083680e+06\n",
      " -9.0598057e+13  0.0000000e+00 -8.6843157e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.2084508e+06 -9.0174390e+13  0.0000000e+00\n",
      " -8.3978229e+13  0.0000000e+00  0.0000000e+00 -3.2085032e+06\n",
      " -8.9870127e+13  0.0000000e+00 -8.4611669e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.2062780e+06 -9.0597663e+13  0.0000000e+00\n",
      " -8.6264359e+13  0.0000000e+00  0.0000000e+00 -3.2060368e+06\n",
      " -9.1696948e+13  0.0000000e+00 -8.0914508e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.2052895e+06 -9.0528843e+13  0.0000000e+00\n",
      " -7.2790577e+13  0.0000000e+00  0.0000000e+00 -3.2035795e+06\n",
      " -8.9832253e+13  0.0000000e+00 -9.1955292e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.2031435e+06 -8.9004649e+13  0.0000000e+00\n",
      " -8.1493825e+13  0.0000000e+00  0.0000000e+00 -3.2056780e+06\n",
      " -9.1081845e+13  0.0000000e+00 -8.6446837e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.2046735e+06 -9.1321591e+13  0.0000000e+00\n",
      " -8.3777783e+13  0.0000000e+00  0.0000000e+00 -3.2035445e+06\n",
      " -9.0184616e+13  0.0000000e+00 -1.0223858e+14  0.0000000e+00\n",
      "  0.0000000e+00 -3.2027098e+06 -9.0630387e+13  0.0000000e+00\n",
      " -8.8005600e+13  0.0000000e+00  0.0000000e+00 -3.2028285e+06\n",
      " -9.0935237e+13  0.0000000e+00 -8.7366094e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.2008332e+06 -9.1725888e+13  0.0000000e+00\n",
      " -7.3404447e+13  0.0000000e+00  0.0000000e+00 -3.2033262e+06\n",
      " -9.1298145e+13  0.0000000e+00 -8.0960285e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.2042802e+06 -1.2201623e+14  0.0000000e+00\n",
      " -1.1951048e+14  0.0000000e+00  0.0000000e+00 -3.2050112e+06\n",
      " -1.6370488e+14  0.0000000e+00 -9.2821516e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.2046252e+06 -1.9567905e+14  0.0000000e+00\n",
      " -6.7574570e+13  0.0000000e+00  0.0000000e+00 -3.2031382e+06\n",
      " -2.1335032e+14  0.0000000e+00 -8.7218790e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.2029705e+06 -2.3102158e+14  0.0000000e+00\n",
      " -8.1806141e+13  0.0000000e+00  0.0000000e+00 -3.2023240e+06\n",
      " -2.4869285e+14  0.0000000e+00 -1.2389310e+14  0.0000000e+00\n",
      "  0.0000000e+00 -3.1997742e+06 -2.6410641e+14  0.0000000e+00\n",
      " -8.0011105e+13  0.0000000e+00  0.0000000e+00 -3.2010768e+06\n",
      " -2.6520195e+14  0.0000000e+00 -7.2663523e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.2013468e+06 -2.4514876e+14  0.0000000e+00\n",
      " -7.9387429e+13  0.0000000e+00  0.0000000e+00 -3.1985405e+06\n",
      " -2.2509558e+14  0.0000000e+00 -8.3987205e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1974155e+06 -2.0504241e+14  0.0000000e+00\n",
      " -1.0198516e+14  0.0000000e+00  0.0000000e+00 -3.1957045e+06\n",
      " -1.8106289e+14  0.0000000e+00 -8.6020377e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1959172e+06 -1.3230246e+14  0.0000000e+00\n",
      " -8.7881415e+13  0.0000000e+00  0.0000000e+00 -3.1962025e+06\n",
      " -8.9010714e+13  0.0000000e+00 -8.0883218e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1960522e+06 -8.7944891e+13  0.0000000e+00\n",
      " -7.3415017e+13  0.0000000e+00  0.0000000e+00 -3.1963285e+06\n",
      " -9.8941249e+13  0.0000000e+00 -7.3142706e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1953215e+06 -8.8322622e+13  0.0000000e+00\n",
      " -8.0233336e+13  0.0000000e+00  0.0000000e+00 -3.1964590e+06\n",
      " -8.8663191e+13  0.0000000e+00 -8.7340324e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1962810e+06 -9.7956921e+13  0.0000000e+00\n",
      " -8.8581956e+13  0.0000000e+00  0.0000000e+00 -3.1965768e+06\n",
      " -9.7451667e+13  0.0000000e+00 -8.7389876e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1959818e+06 -9.9315003e+13  0.0000000e+00\n",
      " -8.8427849e+13  0.0000000e+00  0.0000000e+00 -3.1955038e+06\n",
      " -9.9259672e+13  0.0000000e+00 -8.0096535e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1945652e+06 -9.8910622e+13  0.0000000e+00\n",
      " -8.8570530e+13  0.0000000e+00  0.0000000e+00 -3.1953950e+06\n",
      " -9.8107379e+13  0.0000000e+00 -1.1345752e+14  0.0000000e+00\n",
      "  0.0000000e+00 -3.1938615e+06 -9.9369840e+13  0.0000000e+00\n",
      " -8.7777580e+13  0.0000000e+00  0.0000000e+00 -3.1930645e+06\n",
      " -9.8496644e+13  0.0000000e+00 -8.1563627e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1918372e+06 -9.8597543e+13  0.0000000e+00\n",
      " -9.4438060e+13  0.0000000e+00  0.0000000e+00 -3.1899000e+06\n",
      " -9.7333631e+13  0.0000000e+00 -8.6767399e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1890745e+06 -9.7453143e+13  0.0000000e+00\n",
      " -8.0460030e+13  0.0000000e+00  0.0000000e+00 -3.1871760e+06\n",
      " -9.8545642e+13  0.0000000e+00 -9.0200445e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1892715e+06 -9.8097531e+13  0.0000000e+00\n",
      " -7.5304266e+13  0.0000000e+00  0.0000000e+00 -3.1874015e+06\n",
      " -9.8486838e+13  0.0000000e+00 -8.7434855e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1883208e+06 -9.8125700e+13  0.0000000e+00\n",
      " -7.5150989e+13  0.0000000e+00  0.0000000e+00 -3.1881228e+06\n",
      " -9.7775451e+13  0.0000000e+00 -7.9434011e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1889388e+06 -9.6981209e+13  0.0000000e+00\n",
      " -6.7185725e+13  0.0000000e+00  0.0000000e+00 -3.1889285e+06\n",
      " -9.6627788e+13  0.0000000e+00 -8.7494398e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1905250e+06 -9.8240574e+13  0.0000000e+00\n",
      " -8.8102144e+13  0.0000000e+00  0.0000000e+00 -3.1884368e+06\n",
      " -9.7337767e+13  0.0000000e+00 -8.6625447e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1866402e+06 -9.6623946e+13  0.0000000e+00\n",
      " -7.8523352e+13  0.0000000e+00  0.0000000e+00 -3.1879670e+06\n",
      " -9.8193740e+13  0.0000000e+00 -8.0801765e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1880608e+06 -9.5748075e+13  0.0000000e+00\n",
      " -7.8345077e+13  0.0000000e+00  0.0000000e+00 -3.1880420e+06\n",
      " -9.5964929e+13  0.0000000e+00 -8.5951942e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1865658e+06 -9.7440905e+13  0.0000000e+00\n",
      " -8.5267281e+13  0.0000000e+00  0.0000000e+00 -3.1858718e+06\n",
      " -9.5423579e+13  0.0000000e+00 -8.9678087e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1849690e+06 -9.5210525e+13  0.0000000e+00\n",
      " -8.7429554e+13  0.0000000e+00  0.0000000e+00 -3.1851952e+06\n",
      " -9.5523940e+13  0.0000000e+00 -7.2652014e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1850082e+06 -9.7055238e+13  0.0000000e+00\n",
      " -8.6347608e+13  0.0000000e+00  0.0000000e+00 -3.1819218e+06\n",
      " -9.6888523e+13  0.0000000e+00 -1.0141521e+14  0.0000000e+00\n",
      "  0.0000000e+00 -3.1816780e+06 -9.5819974e+13  0.0000000e+00\n",
      " -8.0839598e+13  0.0000000e+00  0.0000000e+00 -3.1804355e+06\n",
      " -9.5481703e+13  0.0000000e+00 -8.5998424e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1822060e+06 -9.6277103e+13  0.0000000e+00\n",
      " -8.8216145e+13  0.0000000e+00  0.0000000e+00 -3.1810822e+06\n",
      " -9.5833287e+13  0.0000000e+00 -8.6805114e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1807418e+06 -9.5494555e+13  0.0000000e+00\n",
      " -6.7571701e+13  0.0000000e+00  0.0000000e+00 -3.1810220e+06\n",
      " -9.5508303e+13  0.0000000e+00 -8.8669952e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1786895e+06 -9.5475689e+13  0.0000000e+00\n",
      " -9.0669729e+13  0.0000000e+00  0.0000000e+00 -3.1813210e+06\n",
      " -9.3926875e+13  0.0000000e+00 -6.5384896e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1804740e+06 -9.4840965e+13  0.0000000e+00\n",
      " -8.1228921e+13  0.0000000e+00  0.0000000e+00 -3.1786535e+06\n",
      " -9.4742365e+13  0.0000000e+00 -8.3164014e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1786658e+06 -9.4345324e+13  0.0000000e+00\n",
      " -7.0932920e+13  0.0000000e+00  0.0000000e+00 -3.1776570e+06\n",
      " -1.6939834e+14  0.0000000e+00 -8.0098020e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1771538e+06 -4.2957166e+14  0.0000000e+00\n",
      " -8.4269205e+13  0.0000000e+00  0.0000000e+00 -3.1780218e+06\n",
      " -6.1130988e+14  0.0000000e+00 -8.6575635e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1766100e+06 -5.8247917e+14  0.0000000e+00\n",
      " -9.1737482e+13  0.0000000e+00  0.0000000e+00 -3.1762365e+06\n",
      " -4.5620535e+14  0.0000000e+00 -1.6826316e+14  0.0000000e+00\n",
      "  0.0000000e+00 -3.1759325e+06 -3.6155783e+14  0.0000000e+00\n",
      " -2.6998047e+14  0.0000000e+00  0.0000000e+00 -3.1755530e+06\n",
      " -2.8353710e+14  0.0000000e+00 -3.1516222e+14  0.0000000e+00\n",
      "  0.0000000e+00 -3.1741445e+06 -2.1690575e+14  0.0000000e+00\n",
      " -2.8200487e+14  0.0000000e+00  0.0000000e+00 -3.1767218e+06\n",
      " -1.6349571e+14  0.0000000e+00 -2.1685575e+14  0.0000000e+00\n",
      "  0.0000000e+00 -3.1728768e+06 -1.2383184e+14  0.0000000e+00\n",
      " -1.6426785e+14  0.0000000e+00  0.0000000e+00 -3.1704095e+06\n",
      " -9.3544597e+13  0.0000000e+00 -1.2780423e+14  0.0000000e+00\n",
      "  0.0000000e+00 -3.1689948e+06 -9.3227089e+13  0.0000000e+00\n",
      " -8.8628882e+13  0.0000000e+00  0.0000000e+00 -3.1694120e+06\n",
      " -9.3201990e+13  0.0000000e+00 -9.5590269e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1702748e+06 -9.3962593e+13  0.0000000e+00\n",
      " -8.7464971e+13  0.0000000e+00  0.0000000e+00 -3.1674902e+06\n",
      " -9.2747562e+13  0.0000000e+00 -9.2023642e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1681030e+06 -9.2858107e+13  0.0000000e+00\n",
      " -8.0800347e+13  0.0000000e+00  0.0000000e+00 -3.1692295e+06\n",
      " -9.2415516e+13  0.0000000e+00 -8.8420131e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.1685038e+06 -9.2070149e+13  0.0000000e+00\n",
      " -9.2316656e+13  0.0000000e+00  0.0000000e+00 -3.1696090e+06\n",
      " -9.1304906e+13  0.0000000e+00 -8.5059873e+13  0.0000000e+00]\n",
      "Global feature max (train set only): [7.5028229e+00 4.4078564e+03 6.5458751e+17 7.9735429e+17 2.2450910e+18\n",
      " 2.3698273e+02 7.5037384e+00 5.0981353e+03 6.1534924e+17 7.9877060e+17\n",
      " 1.2322810e+18 2.4030396e+02 7.5353251e+00 5.2065059e+03 5.9603742e+17\n",
      " 8.0080758e+17 4.0312436e+17 2.4072734e+02 7.5318155e+00 3.9068164e+03\n",
      " 6.5520111e+17 8.0231068e+17 4.6291127e+17 2.5956421e+02 7.5292211e+00\n",
      " 4.8555830e+03 6.5481546e+17 7.9893614e+17 4.9665332e+17 2.6361975e+02\n",
      " 7.5542469e+00 5.5654751e+03 6.5458751e+17 7.9678859e+17 4.9665332e+17\n",
      " 5.1636298e+02 7.5493641e+00 4.9293604e+03 6.5458751e+17 7.9318102e+17\n",
      " 4.9665332e+17 4.5296130e+02 7.5818658e+00 4.9896997e+03 6.5458751e+17\n",
      " 7.8922154e+17 4.9665332e+17 3.1719446e+02 7.5827813e+00 4.3936216e+03\n",
      " 6.5458751e+17 7.8526200e+17 4.9665332e+17 2.8778918e+02 7.5919371e+00\n",
      " 5.3535103e+03 6.5458751e+17 7.8130252e+17 4.9665332e+17 2.9527701e+02\n",
      " 7.6006346e+00 4.5104268e+03 6.5458751e+17 7.8181118e+17 4.9665332e+17\n",
      " 2.5331425e+02 7.5957522e+00 4.8942114e+03 6.5458751e+17 7.8424048e+17\n",
      " 4.9665332e+17 2.5532495e+02 7.9549546e+00 7.2445923e+03 6.6080738e+17\n",
      " 7.8666978e+17 4.9665332e+17 4.4181900e+02 7.9738760e+00 4.5760366e+03\n",
      " 6.6070348e+17 7.8909909e+17 4.9665332e+17 4.3258957e+02 7.9120760e+00\n",
      " 4.4441621e+03 6.6090909e+17 7.9152846e+17 4.9665332e+17 3.8798080e+02\n",
      " 7.8362379e+00 5.3632007e+03 6.6123537e+17 7.9397487e+17 5.0893998e+17\n",
      " 3.2029855e+02 7.7787104e+00 4.3676050e+03 6.6158645e+17 7.9867549e+17\n",
      " 5.4605609e+17 2.8799564e+02 7.7443767e+00 4.3840063e+03 6.6191335e+17\n",
      " 8.0353664e+17 5.4149137e+17 2.7672519e+02 7.7550588e+00 4.6990625e+03\n",
      " 6.6219510e+17 8.0663004e+17 5.2302277e+17 2.8401221e+02 7.6480908e+00\n",
      " 4.6085703e+03 6.6241342e+17 8.0952945e+17 5.1442854e+17 3.7183932e+02\n",
      " 7.6761684e+00 4.8184873e+03 6.6260605e+17 8.1242887e+17 5.2705001e+17\n",
      " 3.5334946e+02 7.6734219e+00 4.9185420e+03 6.6278279e+17 8.1532821e+17\n",
      " 4.9674303e+17 2.5074829e+02 7.6911221e+00 5.2035127e+03 6.6292236e+17\n",
      " 8.1822762e+17 4.9691700e+17 3.8601617e+02 7.7098908e+00 4.7377520e+03\n",
      " 6.3488784e+17 8.2078790e+17 4.7185892e+17 4.0046399e+02 7.6940217e+00\n",
      " 4.9858984e+03 5.3849681e+17 8.2026089e+17 4.7048738e+17 2.9831079e+02\n",
      " 7.7098908e+00 4.2470435e+03 5.5308441e+17 8.1939503e+17 4.7032503e+17\n",
      " 2.3346985e+02 7.7030244e+00 5.0357173e+03 6.3185786e+17 8.1852916e+17\n",
      " 4.7419665e+17 2.2393640e+02 7.7353740e+00 4.5652183e+03 6.1233699e+17\n",
      " 8.1766337e+17 4.7422112e+17 2.2525632e+02 7.7269816e+00 4.6530869e+03\n",
      " 4.9839014e+17 8.1679750e+17 4.7423520e+17 2.2799617e+02 7.7478867e+00\n",
      " 4.9667437e+03 6.5348498e+17 8.1614879e+17 7.8739629e+17 2.3528183e+02\n",
      " 7.7474289e+00 5.0100244e+03 6.5319773e+17 8.1748841e+17 7.8732379e+17\n",
      " 2.6404922e+02 7.7745905e+00 4.2986089e+03 6.5319773e+17 8.1904635e+17\n",
      " 1.7011927e+18 2.3466342e+02 7.7738276e+00 4.9129404e+03 6.5319773e+17\n",
      " 8.2060429e+17 1.8753850e+18 2.6636292e+02 7.7929015e+00 4.5587534e+03\n",
      " 6.5323395e+17 8.2216222e+17 9.5215487e+17 3.7343405e+02 7.7755055e+00\n",
      " 4.9531753e+03 6.5362097e+17 8.2372016e+17 6.1209338e+17 3.8725525e+02\n",
      " 7.7634511e+00 5.4494731e+03 6.5280081e+17 8.2554398e+17 4.7121804e+17\n",
      " 4.2331500e+02 7.7838984e+00 6.0167583e+03 6.2584896e+17 8.2981613e+17\n",
      " 1.8393942e+18 4.0901456e+02 7.7758107e+00 5.2348472e+03 6.4304292e+17\n",
      " 8.3435711e+17 1.5315800e+18 3.5606204e+02 7.7860346e+00 4.5206812e+03\n",
      " 4.4865493e+17 8.3889803e+17 4.7010097e+17 4.2691904e+02 7.7825251e+00\n",
      " 4.1297002e+03 4.1326426e+17 8.4343901e+17 4.7015932e+17 4.2682309e+02\n",
      " 7.7938166e+00 4.8932168e+03 3.6615809e+17 8.4797999e+17 4.7026738e+17\n",
      " 4.7320395e+02 7.7982421e+00 5.2583672e+03 3.2216519e+17 8.5139590e+17\n",
      " 4.7081009e+17 4.8836819e+02 7.8232675e+00 4.5997461e+03 3.4336669e+17\n",
      " 8.4439091e+17 4.7153003e+17 3.7316583e+02 7.8179264e+00 4.4583872e+03\n",
      " 3.7681500e+17 8.3624195e+17 4.7156205e+17 3.7428888e+02 7.8434095e+00\n",
      " 4.6811528e+03 3.6156722e+17 8.2809299e+17 4.7159164e+17 4.1135327e+02\n",
      " 7.8278451e+00 4.8471226e+03 3.4282965e+17 8.1994403e+17 4.7162266e+17\n",
      " 4.1457623e+02 7.8458509e+00 4.7348481e+03 3.1290486e+17 8.1179507e+17\n",
      " 4.3968182e+17 3.7316583e+02 7.8264718e+00 5.6483345e+03 3.1916826e+17\n",
      " 8.0396799e+17 4.5759887e+17 3.1114746e+02 7.8423414e+00 4.4485889e+03\n",
      " 3.4712228e+17 7.9914017e+17 4.7014080e+17 2.3335298e+02 7.8492084e+00\n",
      " 4.3094507e+03 3.3565118e+17 7.9464165e+17 4.7014080e+17 2.3936926e+02\n",
      " 7.8553119e+00 5.1340728e+03 2.8326400e+17 7.9014307e+17 4.7014080e+17\n",
      " 2.5720020e+02 7.7405624e+00 4.5989790e+03 2.7739410e+17 7.8564456e+17\n",
      " 4.7014080e+17 3.8922922e+02 7.7466660e+00 4.7228325e+03 3.3062119e+17\n",
      " 7.8114604e+17 4.6336055e+17 3.8401514e+02 7.7657399e+00 4.3810181e+03\n",
      " 3.8402491e+17 7.7705119e+17 4.4075978e+17 3.2798047e+02 7.7565842e+00\n",
      " 4.6208213e+03 4.5390493e+17 7.7658362e+17 4.0312591e+17 2.9409357e+02\n",
      " 6.9882812e+00 5.5517197e+03 5.2561569e+17 7.7651408e+17 4.0312591e+17\n",
      " 2.8900284e+02 7.2806482e+00 5.4305674e+03 5.5243065e+17 7.7644446e+17\n",
      " 4.0312591e+17 2.6439758e+02 7.0162053e+00 5.1904678e+03 5.7425712e+17\n",
      " 7.7950612e+17 4.2043249e+17 2.4909766e+02 6.9989624e+00 5.0243184e+03\n",
      " 5.7562052e+17 7.8383380e+17 4.3705725e+17 2.3983968e+02 7.0300913e+00\n",
      " 4.3857075e+03 5.7471019e+17 7.8816148e+17 4.4834253e+17 2.4306415e+02\n",
      " 7.0531325e+00 5.0740552e+03 5.6773637e+17 7.9248922e+17 4.4576833e+17\n",
      " 2.4668184e+02 7.0763268e+00 5.3663940e+03 5.6008882e+17 7.9616874e+17\n",
      " 4.4146732e+17 2.4738966e+02 7.0720544e+00 5.5345845e+03 5.5844618e+17\n",
      " 7.9391522e+17 4.6732519e+17 2.4854752e+02 7.0769372e+00 4.5545757e+03\n",
      " 5.5747074e+17 7.9101045e+17 4.7031194e+17 2.7410007e+02 7.1036410e+00\n",
      " 4.8195972e+03 5.8261857e+17 7.8810568e+17 4.7034960e+17 2.9094150e+02\n",
      " 7.0920439e+00 4.3602622e+03 6.1066896e+17 7.8520084e+17 4.7039440e+17\n",
      " 2.9442593e+02 7.1324811e+00 4.2133320e+03 6.3269204e+17 7.8229606e+17\n",
      " 4.7836387e+17 2.8776566e+02 7.1282082e+00 5.5849619e+03 6.5404539e+17\n",
      " 7.7923853e+17 4.8583285e+17 2.8406863e+02 7.1456037e+00 4.4796245e+03\n",
      " 5.1466229e+17 7.7477485e+17 4.4735349e+17 2.6619962e+02 7.1478925e+00\n",
      " 5.6521685e+03 3.5741931e+17 7.7015684e+17 3.6922453e+17 2.3208229e+02\n",
      " 7.1707816e+00 6.3689644e+03 2.7295223e+17 7.6553882e+17 3.6922453e+17\n",
      " 2.3208229e+02 7.1762753e+00 5.8251411e+03 3.5286087e+17 7.6424208e+17\n",
      " 3.6922453e+17 2.3208229e+02 7.1851254e+00 4.5996538e+03 3.8802459e+17\n",
      " 7.6658301e+17 3.6922453e+17 2.3546515e+02 7.1742911e+00 6.2909043e+03\n",
      " 6.5645833e+17 7.6892387e+17 3.6922453e+17 2.3664850e+02 7.1868038e+00\n",
      " 5.9381279e+03 6.4479629e+17 7.7126480e+17 3.6922453e+17 2.3266083e+02\n",
      " 7.1985536e+00 4.6553652e+03 5.8328618e+17 7.7388102e+17 3.6922453e+17\n",
      " 2.4578050e+02 7.2290721e+00 4.7855327e+03 5.1751906e+17 7.7899499e+17\n",
      " 3.6922453e+17 2.8818234e+02 7.2249522e+00 5.4417827e+03 5.2561153e+17\n",
      " 7.8438314e+17 3.6922453e+17 2.7735864e+02 7.2354808e+00 6.3028794e+03\n",
      " 5.8199617e+17 7.8977123e+17 3.9302813e+17 2.8241064e+02 7.2681360e+00\n",
      " 4.3424834e+03 6.3383705e+17 7.9515939e+17 3.6922453e+17 3.7597409e+02\n",
      " 7.2739344e+00 4.4781270e+03 6.5608147e+17 8.0054748e+17 3.7650188e+17\n",
      " 4.5006165e+02 7.2801905e+00 4.9894219e+03 6.5625664e+17 8.0517511e+17\n",
      " 3.8844605e+17 4.9961023e+02 7.2788172e+00 4.8240356e+03 6.5650431e+17\n",
      " 8.0286456e+17 3.9317457e+17 5.2325842e+02 7.3096404e+00 6.0209165e+03\n",
      " 6.5663824e+17 7.9979245e+17 3.9317457e+17 4.5468759e+02 7.3078094e+00\n",
      " 4.6690176e+03 6.5670284e+17 7.9672035e+17 3.9717975e+17 4.4000314e+02\n",
      " 7.3357339e+00 4.4057124e+03 6.5682976e+17 7.9364825e+17 3.9317457e+17\n",
      " 4.5116922e+02 7.3294778e+00 4.3905967e+03 6.1849515e+17 7.9057614e+17\n",
      " 4.5661526e+17 2.4150473e+02 7.3363447e+00 4.9231182e+03 5.1747322e+17\n",
      " 7.8775466e+17 4.7092598e+17 2.1908818e+02 7.3668628e+00 4.7917354e+03\n",
      " 4.7792877e+17 7.8723260e+17 4.7092598e+17 2.3368826e+02 7.3645740e+00\n",
      " 5.1364355e+03 3.6268676e+17 7.8696294e+17 4.7092598e+17 2.3561514e+02\n",
      " 7.3570971e+00 4.8376021e+03 2.5448580e+17 7.8669322e+17 4.7092598e+17\n",
      " 2.4323296e+02 7.3833427e+00 4.8759971e+03 2.3145378e+17 7.8642356e+17\n",
      " 4.7092598e+17 2.5983231e+02 7.3894467e+00 5.9386943e+03 2.0867011e+17\n",
      " 7.8615384e+17 4.7092598e+17 2.6627808e+02 7.3981447e+00 4.7211113e+03\n",
      " 6.2548481e+17 7.8607880e+17 1.4398046e+18 2.9024918e+02 7.4176764e+00\n",
      " 3.9837725e+03 4.1406161e+17 7.8780008e+17 5.6515609e+17 3.3635941e+02\n",
      " 7.4164557e+00 4.7424688e+03 1.2220701e+17 7.8971859e+17 4.0539296e+17\n",
      " 3.6648914e+02 7.4390392e+00 5.9464858e+03 1.5315690e+17 7.9163703e+17\n",
      " 4.0312436e+17 3.7273550e+02 7.4515519e+00 4.3601782e+03 2.7612685e+17\n",
      " 7.9355554e+17 4.7101717e+17 3.3189801e+02 7.4701681e+00 4.3239922e+03\n",
      " 4.9782942e+17 7.9547405e+17 1.0108969e+18 2.4626016e+02 7.4694052e+00\n",
      " 4.0499595e+03 6.5544025e+17 7.9721905e+17 1.8856209e+18 2.2111931e+02]\n"
     ]
    }
   ],
   "source": [
    "# This cell takes 19s\n",
    "# Instantiate the dataset without transformations\n",
    "dataset = IndependentCSVDataset(path, features_list)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "feature_min, feature_max = compute_global_minmax(train_dataset)\n",
    "print(\"Global feature min (train set only):\", feature_min)\n",
    "print(\"Global feature max (train set only):\", feature_max)\n",
    "\n",
    "global_transform = GlobalMinMaxNormalize(feature_min, feature_max)\n",
    "train_dataset.dataset.transform = global_transform\n",
    "test_dataset.dataset.transform = global_transform  # Same transform to prevent data leakage\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project\\Models\\helpers.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample = torch.tensor(sample, dtype=torch.float32)  # (60, 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([32, 60, 600])\n",
      "Targets shape: torch.Size([32, 60, 100])\n"
     ]
    }
   ],
   "source": [
    "# Create an iterator from the DataLoader\n",
    "train_iterator = iter(train_loader)\n",
    "\n",
    "# Get a single batch\n",
    "batch = next(train_iterator)\n",
    "\n",
    "# Unpack the batch (assuming your dataset returns (inputs, targets))\n",
    "inputs, targets = batch\n",
    "\n",
    "print(\"Inputs shape:\", inputs.shape)\n",
    "print(\"Targets shape:\", targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [1397912.0, 18088.0]\n",
      "Class ratio: 77.28394317626953\n",
      "LSTMModel(\n",
      "  (lstm): LSTM(600, 400, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (fc1): Linear(in_features=400, out_features=400, bias=True)\n",
      "  (fc2): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project\\Models\\helpers.py:230: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(class_counts[0] / class_counts[1], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 60, 600])\n",
      "Output shape: torch.Size([32, 60, 100])\n",
      "Targets shape: torch.Size([32, 60, 100])\n",
      "1.0589429140090942\n"
     ]
    }
   ],
   "source": [
    "# Define loss and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=compute_class_weights(train_loader))\n",
    "# Define the model and print the architecture, example with dummy input shapes\n",
    "\n",
    "input_size = 600       # Number of features per time step\n",
    "hidden_size = 400     # Number of LSTM hidden units\n",
    "num_layers = 2       # Number of LSTM layers\n",
    "output_size = 100      # Binary classification\n",
    "dropout = 0.2\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "\n",
    "# Print model architecture\n",
    "print(model)\n",
    "\n",
    "# Example dummy input: batch_size=32, sequence_length=6000, input_size=6\n",
    "dummy_input = inputs\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "print(\"Input shape:\", dummy_input.shape)  # Expected: (32, 6000, 6) -> 6 features per timestep\n",
    "print(\"Output shape:\", dummy_output.shape)  # Expected: (32, 6000,) -> one binary value per timestep\n",
    "print(\"Targets shape:\", targets.shape)\n",
    "\n",
    "loss = criterion(dummy_output, targets)\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project\\Models\\helpers.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample = torch.tensor(sample, dtype=torch.float32)  # (60, 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 5.1517.2139, batch_idx=7\n",
      "Epoch [2/20], Loss: 1.3213.5708, batch_idx=7\n",
      "Accumulated epoch loss: 1.8132, batch_idx=1\r"
     ]
    }
   ],
   "source": [
    "# Define the optimizer\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-2)\n",
    "\n",
    "# Assume `train_loader` is a DataLoader yielding batches of (input, target)\n",
    "num_epochs = 20\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        print(f'Accumulated epoch loss: {epoch_loss:.4f}, {batch_idx=}', end='\\r')\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 8.1832\n",
      "F1 Score: 0.0170\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate_test_metrics(model, test_loader, criterion, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluates the test loss, F1-score, and ROC-AUC of a trained model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained PyTorch model.\n",
    "    - test_loader: DataLoader for the test dataset.\n",
    "    - criterion: Loss function (e.g., nn.BCEWithLogitsLoss).\n",
    "    - device: The device ('cuda' or 'cpu').\n",
    "    - threshold: Decision threshold for binary classification (default=0.5).\n",
    "\n",
    "    Returns:\n",
    "    - Average test loss\n",
    "    - F1-score\n",
    "    - ROC-AUC score\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    total_samples = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to device\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)  # (batch_size, 6000) -> raw logits\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)  # BCEWithLogitsLoss expects raw logits\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            # Convert logits to probabilities using Sigmoid\n",
    "            probs = torch.sigmoid(outputs)  # (batch_size, 6000) -> probabilities in [0,1]\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            preds = (probs > threshold).int()  # Convert to 0 or 1\n",
    "\n",
    "            # Flatten for metric calculations\n",
    "            all_targets.extend(targets.cpu().numpy().flatten())  # Convert to 1D list\n",
    "            all_predictions.extend(preds.cpu().numpy().flatten())  # Convert to 1D list\n",
    "\n",
    "    # Compute average loss\n",
    "    avg_test_loss = test_loss / total_samples\n",
    "\n",
    "    # Compute F1-score\n",
    "    f1 = f1_score(all_targets, all_predictions, average='macro')\n",
    "\n",
    "    return avg_test_loss, f1\n",
    "\n",
    "# Example Usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_loss, test_f1 = evaluate_test_metrics(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
