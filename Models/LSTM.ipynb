{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project')\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "%matplotlib widget\n",
    "import pandas as pd # for data manipulation\n",
    "from Models.load_data import *\n",
    "from Models.helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "\n",
    "path = r'C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project\\Training_data'\n",
    "features_list = os.listdir(os.path.join(path, r'features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping JETno97612.csv: sequence length 6001 is unexpected.\n",
      "Skipping JETno87125.csv: sequence length 6001 is unexpected.\n",
      "Skipping JETno91071.csv: sequence length 6001 is unexpected.\n",
      "Skipping JETno99471.csv: sequence length 6001 is unexpected.\n",
      "Total sliding window samples created: 176712\n",
      "Global feature min (train set only): [ 0.0000000e+00 -3.5154782e+06 -6.1130988e+14  0.0000000e+00\n",
      " -3.1516222e+14  0.0000000e+00]\n",
      "Global feature max (train set only): [7.9738760e+00 7.2445923e+03 6.6544650e+17 8.5139590e+17 2.2450910e+18\n",
      " 1.6388709e+04]\n"
     ]
    }
   ],
   "source": [
    "# This cell takes 19s\n",
    "# Instantiate the dataset without transformations\n",
    "dataset = IndependentCSVDataset(path, features_list)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "feature_min, feature_max = compute_global_minmax(train_dataset)\n",
    "print(\"Global feature min (train set only):\", feature_min)\n",
    "print(\"Global feature max (train set only):\", feature_max)\n",
    "\n",
    "global_transform = GlobalMinMaxNormalize(feature_min, feature_max)\n",
    "train_dataset.dataset.transform = global_transform\n",
    "test_dataset.dataset.transform = global_transform  # Same transform to prevent data leakage\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([64, 40, 6])\n",
      "Targets shape: torch.Size([64, 40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project\\Models\\helpers.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Create an iterator from the DataLoader\n",
    "train_iterator = iter(train_loader)\n",
    "\n",
    "# Get a single batch\n",
    "batch = next(train_iterator)\n",
    "\n",
    "# Unpack the batch (assuming your dataset returns (inputs, targets))\n",
    "inputs, targets = batch\n",
    "\n",
    "print(\"Inputs shape:\", inputs.shape)\n",
    "print(\"Targets shape:\", targets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm): LSTM(6, 40, num_layers=2, batch_first=True, dropout=0.31159803520368773)\n",
      "  (fc1): Linear(in_features=40, out_features=40, bias=True)\n",
      "  (fc2): Linear(in_features=40, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Input shape: torch.Size([64, 40, 6])\n",
      "Output shape: torch.Size([64, 40])\n",
      "Targets shape: torch.Size([64, 40])\n",
      "0.7858231067657471\n"
     ]
    }
   ],
   "source": [
    "# Define loss and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=compute_class_weights(train_loader))\n",
    "# Define the model and print the architecture, example with dummy input shapes\n",
    "# [I 2025-02-07 01:30:51,832] Trial 37 finished with value: 1.143294748031732 and parameters: \n",
    "# {'hidden_size': 401, 'dropout': 0.31159803520368773, 'learning_rate': 0.0004282581786099833, \n",
    "# 'weight_decay': 3.0769712937166838e-06}. Best is trial 37 with value: 1.143294748031732.\n",
    "\n",
    "input_size = 6       # Number of features per time step\n",
    "hidden_size = 40     # Number of LSTM hidden units\n",
    "num_layers = 2       # Number of LSTM layers\n",
    "output_size = 1      # Binary classification\n",
    "dropout = 0.31159803520368773\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "\n",
    "# Print model architecture\n",
    "print(model)\n",
    "\n",
    "# Example dummy input: batch_size=32, sequence_length=6000, input_size=6\n",
    "dummy_input = inputs\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "print(\"Input shape:\", dummy_input.shape)  # Expected: (32, 6000, 6) -> 6 features per timestep\n",
    "print(\"Output shape:\", dummy_output.shape)  # Expected: (32, 6000,) -> one binary value per timestep\n",
    "print(\"Targets shape:\", targets.shape)\n",
    "\n",
    "loss = criterion(dummy_output, targets)\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project\\Models\\helpers.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class LSTMModel(nn.Module):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Loss: 0.096913.9431, batch_idx=2208\n",
      "Epoch [2/2], Loss: 0.073061.3136, batch_idx=2208\n",
      "Model saved at C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project\\Models\\lstm_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0004282581786099833, weight_decay=3.0769712937166838e-06)\n",
    "\n",
    "# Assume `train_loader` is a DataLoader yielding batches of (input, target)\n",
    "num_epochs = 2\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        print(f'Accumulated epoch loss: {epoch_loss:.4f}, {batch_idx=}', end='\\r')\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), r'C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project\\Models\\lstm_model.pth')\n",
    "print('Model saved at C:\\\\Users\\\\Max Tost\\\\Desktop\\\\Notebooks\\\\SPC Neural Network Project\\\\Models\\\\lstm_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting outputs of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(600, 401, num_layers=2, batch_first=True, dropout=0.31159803520368773)\n",
       "  (fc1): Linear(in_features=401, out_features=401, bias=True)\n",
       "  (fc2): Linear(in_features=401, out_features=100, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters:\n",
    "input_size = 600       # Number of features per time step\n",
    "hidden_size = 401     # Number of LSTM hidden units\n",
    "num_layers = 2       # Number of LSTM layers\n",
    "output_size = 100      # Binary classification\n",
    "dropout = 0.31159803520368773\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "model.load_state_dict(torch.load(r'C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project\\Models\\lstm_model.pth', weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([60, 600])\n",
      "Targets shape: torch.Size([60, 100])\n",
      "Outputs shape: torch.Size([60, 100])\n"
     ]
    }
   ],
   "source": [
    "# Create an iterator from the DataLoader\n",
    "train_iterator = iter(train_loader)\n",
    "\n",
    "# Get a single batch\n",
    "batch = next(train_iterator)\n",
    "\n",
    "# Unpack the batch (assuming your dataset returns (inputs, targets))\n",
    "inputs, targets = batch\n",
    "input, target = inputs[0], targets[0]\n",
    "\n",
    "print(\"Inputs shape:\", input.shape)\n",
    "print(\"Targets shape:\", target.shape)\n",
    "\n",
    "output = model(input)\n",
    "\n",
    "print(\"Outputs shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-39632.2656, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([60, 600])\n",
      "Targets shape: torch.Size([60, 100])\n",
      "Outputs shape: torch.Size([60, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23e59f554d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a5212d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2d650>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2b350>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f292d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2a090>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2a710>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f29f50>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f290d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2a3d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f6a7d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a50d3d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2a2d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f29a10>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2ac10>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2bf50>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2bb10>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fdb750>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ef0c10>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fd8990>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fd9f90>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2a690>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fd9e90>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fd9990>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a5235d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ebb050>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f6a810>,\n",
       " <matplotlib.lines.Line2D at 0x23e59feab50>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fea350>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ff27d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4b3550>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fe8990>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4d3010>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a520190>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fd8390>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ef03d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ff8f50>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ff9350>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ff96d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ff9a90>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ff9e90>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a522150>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffa4d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fea810>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffac10>,\n",
       " <matplotlib.lines.Line2D at 0x23e59feaf90>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffb3d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffb7d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffbbd0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4a3f90>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f28390>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a520750>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4e8ad0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fd8c90>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4e9210>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4e9610>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4e99d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fd9d50>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f6a890>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffa450>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffa810>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffab90>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4eafd0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4eb390>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4eb710>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4eb890>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4ebe10>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4e8110>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4e84d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a508710>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a508cd0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a509090>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a509490>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a509850>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f29b90>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a509fd0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2a310>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f6ae90>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a50ab10>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a50ae90>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a50b010>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ddbdd0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a50b9d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a50bdd0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fd80d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4c0550>,\n",
       " <matplotlib.lines.Line2D at 0x23e43190090>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4c0cd0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4c1050>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4c1410>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4b2010>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a521250>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f21e90>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4b2b10>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4c2650>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4c29d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffad90>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4c3110>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fdb450>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4c3850>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4906147fa1a401f85b7dcbaac8befdf",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=640.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 1\n",
    "# Unpack the batch (assuming your dataset returns (inputs, targets))\n",
    "inputs, targets = batch\n",
    "input, target = inputs[k], targets[k]\n",
    "\n",
    "print(\"Inputs shape:\", input.shape)\n",
    "print(\"Targets shape:\", target.shape)\n",
    "\n",
    "output = model(input)\n",
    "\n",
    "print(\"Outputs shape:\", output.shape)\n",
    "target_reshaped = target.reshape(60, 100)\n",
    "target_original = target_reshaped.reshape(6000)\n",
    "target_original\n",
    "plt.close()\n",
    "plt.plot(target, label=\"Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-06 19:12:12,255] A new study created in memory with name: no-name-7d42ee83-9fce-41ff-9fab-897202957042\n",
      "[I 2025-02-06 19:14:22,773] Trial 0 finished with value: 1.4831497163893161 and parameters: {'hidden_size': 68, 'dropout': 0.2796572278236362, 'learning_rate': 0.004450825657499489, 'weight_decay': 0.00027124720064343276}. Best is trial 0 with value: 1.4831497163893161.\n",
      "[I 2025-02-06 19:27:39,277] Trial 1 finished with value: 1.6296354915066404 and parameters: {'hidden_size': 364, 'dropout': 0.49295518977565567, 'learning_rate': 0.006043435602079738, 'weight_decay': 0.007852530176627527}. Best is trial 0 with value: 1.4831497163893161.\n",
      "[I 2025-02-06 19:41:36,139] Trial 2 finished with value: 1.2608569962645957 and parameters: {'hidden_size': 365, 'dropout': 0.4004909576509085, 'learning_rate': 0.0006279622666646788, 'weight_decay': 4.0703178450966685e-06}. Best is trial 2 with value: 1.2608569962645957.\n",
      "[I 2025-02-06 19:49:44,332] Trial 3 finished with value: 1.377703879061068 and parameters: {'hidden_size': 239, 'dropout': 0.25523196879007115, 'learning_rate': 0.0006703771045299565, 'weight_decay': 0.005099303717934173}. Best is trial 2 with value: 1.2608569962645957.\n",
      "[I 2025-02-06 19:59:50,084] Trial 4 finished with value: 1.1867264117939336 and parameters: {'hidden_size': 291, 'dropout': 0.19364281664263827, 'learning_rate': 0.0006865318950616603, 'weight_decay': 1.5849743422819565e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 20:14:45,558] Trial 5 finished with value: 1.5464739893741448 and parameters: {'hidden_size': 383, 'dropout': 0.35655891667507134, 'learning_rate': 0.0010791654331135444, 'weight_decay': 5.632642949339343e-05}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 20:31:39,532] Trial 6 finished with value: 1.4043266576548616 and parameters: {'hidden_size': 424, 'dropout': 0.29520839882397754, 'learning_rate': 0.007300116681659483, 'weight_decay': 4.820065222672352e-05}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 20:39:03,536] Trial 7 finished with value: 1.4162979017563524 and parameters: {'hidden_size': 234, 'dropout': 0.46828539530573987, 'learning_rate': 0.000471375248076495, 'weight_decay': 2.0284562509155787e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 20:43:04,474] Trial 8 finished with value: 1.3916922780989551 and parameters: {'hidden_size': 131, 'dropout': 0.21130225823390503, 'learning_rate': 0.000350828989639619, 'weight_decay': 1.3582661220648346e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 20:57:53,224] Trial 9 finished with value: 1.9863161643074811 and parameters: {'hidden_size': 388, 'dropout': 0.4444613271627782, 'learning_rate': 0.0010716334859724543, 'weight_decay': 2.531461240576309e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 21:18:34,304] Trial 10 finished with value: 1.529993732409863 and parameters: {'hidden_size': 489, 'dropout': 0.11049420527530682, 'learning_rate': 0.00011266595362344224, 'weight_decay': 1.193351584073423e-05}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 21:29:40,660] Trial 11 finished with value: 1.5377962602738753 and parameters: {'hidden_size': 302, 'dropout': 0.39013183529719253, 'learning_rate': 0.0027042732533020235, 'weight_decay': 8.40764641904512e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 21:40:25,698] Trial 12 finished with value: 1.1958037031882163 and parameters: {'hidden_size': 305, 'dropout': 0.17967831202687243, 'learning_rate': 0.00032028752144588294, 'weight_decay': 9.51476883274433e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 21:50:19,079] Trial 13 finished with value: 1.4743621980808923 and parameters: {'hidden_size': 289, 'dropout': 0.16022997818880944, 'learning_rate': 0.0001975179404995467, 'weight_decay': 1.7194929239216046e-05}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 21:56:34,232] Trial 14 finished with value: 1.1908392258413893 and parameters: {'hidden_size': 202, 'dropout': 0.19540239888523192, 'learning_rate': 0.0020725605545255134, 'weight_decay': 1.183548786399151e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 22:02:09,850] Trial 15 finished with value: 2.6620272131938667 and parameters: {'hidden_size': 178, 'dropout': 0.10459915553775946, 'learning_rate': 0.002079839100612295, 'weight_decay': 0.0007903261945980043}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 22:08:24,199] Trial 16 finished with value: 1.314391192587628 and parameters: {'hidden_size': 202, 'dropout': 0.23149704771438512, 'learning_rate': 0.0017466164994190212, 'weight_decay': 1.0082056428120592e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 22:12:32,849] Trial 17 finished with value: 1.4090086134088038 and parameters: {'hidden_size': 134, 'dropout': 0.1664708024501188, 'learning_rate': 0.0015166432904652127, 'weight_decay': 0.0004097597924542043}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 22:20:47,639] Trial 18 finished with value: 1.6273238800042666 and parameters: {'hidden_size': 245, 'dropout': 0.33362628620588053, 'learning_rate': 0.002740754383402328, 'weight_decay': 3.0497066268778455e-05}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 22:25:21,458] Trial 19 finished with value: 1.5643321734838433 and parameters: {'hidden_size': 147, 'dropout': 0.19898091761926945, 'learning_rate': 0.004020581737269553, 'weight_decay': 4.967307145055633e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 22:37:02,847] Trial 20 finished with value: 1.3680042901465406 and parameters: {'hidden_size': 327, 'dropout': 0.14121203979528804, 'learning_rate': 0.00019552987284017213, 'weight_decay': 0.0023975786578493323}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 22:45:53,470] Trial 21 finished with value: 1.5243521167848115 and parameters: {'hidden_size': 264, 'dropout': 0.19135096425878514, 'learning_rate': 0.0003027092096832405, 'weight_decay': 5.036806914431397e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 22:57:35,292] Trial 22 finished with value: 1.5703314289799801 and parameters: {'hidden_size': 329, 'dropout': 0.24576120326944453, 'learning_rate': 0.000764081514388741, 'weight_decay': 2.3474232023620535e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 23:03:42,761] Trial 23 finished with value: 1.369413381209613 and parameters: {'hidden_size': 195, 'dropout': 0.14260601278802285, 'learning_rate': 0.0004085348926440025, 'weight_decay': 1.5759138930149353e-05}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 23:13:07,206] Trial 24 finished with value: 1.361975845324496 and parameters: {'hidden_size': 278, 'dropout': 0.18600871494103438, 'learning_rate': 0.00021988846989253168, 'weight_decay': 1.0468020471234891e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 23:30:57,962] Trial 25 finished with value: 1.439107374645939 and parameters: {'hidden_size': 440, 'dropout': 0.2684794695979295, 'learning_rate': 0.00010378885108671855, 'weight_decay': 6.489923601041209e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 23:42:32,997] Trial 26 finished with value: 1.3793749774410775 and parameters: {'hidden_size': 330, 'dropout': 0.22174650879093105, 'learning_rate': 0.0013756800666160363, 'weight_decay': 2.8854777607586505e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 23:45:08,480] Trial 27 finished with value: 1.3442471728761982 and parameters: {'hidden_size': 88, 'dropout': 0.31833357949709085, 'learning_rate': 0.0006114630422983282, 'weight_decay': 0.00011840208200263025}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 23:52:32,323] Trial 28 finished with value: 2.021361284361896 and parameters: {'hidden_size': 233, 'dropout': 0.13005618991433382, 'learning_rate': 0.0009251902828206253, 'weight_decay': 2.7350667241296795e-05}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 23:54:55,797] Trial 29 finished with value: 1.3747405245166 and parameters: {'hidden_size': 75, 'dropout': 0.16575218713031473, 'learning_rate': 0.00028687650228158767, 'weight_decay': 1.946680281877131e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 23:59:53,495] Trial 30 finished with value: 1.421878374541149 and parameters: {'hidden_size': 169, 'dropout': 0.2890679755560639, 'learning_rate': 0.0032986929509432376, 'weight_decay': 0.00011825709947222332}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-07 00:13:20,467] Trial 31 finished with value: 1.3190291822867926 and parameters: {'hidden_size': 360, 'dropout': 0.3954955521195662, 'learning_rate': 0.0005248569721903933, 'weight_decay': 4.183907582635691e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-07 00:24:00,245] Trial 32 finished with value: 1.2703883833166971 and parameters: {'hidden_size': 310, 'dropout': 0.2330557200395165, 'learning_rate': 0.0008151687050746021, 'weight_decay': 8.850129185980588e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-07 00:40:43,943] Trial 33 finished with value: 1.4980719494531305 and parameters: {'hidden_size': 423, 'dropout': 0.42615643576458045, 'learning_rate': 0.005386392209217655, 'weight_decay': 3.465636411965794e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-07 00:53:58,099] Trial 34 finished with value: 1.2230548169582904 and parameters: {'hidden_size': 363, 'dropout': 0.3484978358927232, 'learning_rate': 0.0005947327829179757, 'weight_decay': 1.631361815308677e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-07 01:07:07,741] Trial 35 finished with value: 1.3612183806929468 and parameters: {'hidden_size': 362, 'dropout': 0.35274076232536977, 'learning_rate': 0.0012215275512285175, 'weight_decay': 1.5353185137594975e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-07 01:15:40,441] Trial 36 finished with value: 1.674701767355472 and parameters: {'hidden_size': 262, 'dropout': 0.2689722992911363, 'learning_rate': 0.0005905254847199388, 'weight_decay': 1.029057863621153e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-07 01:30:51,832] Trial 37 finished with value: 1.143294748031732 and parameters: {'hidden_size': 401, 'dropout': 0.31159803520368773, 'learning_rate': 0.0004282581786099833, 'weight_decay': 3.0769712937166838e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 02:10:40,598] Trial 38 finished with value: 1.3947310707237044 and parameters: {'hidden_size': 504, 'dropout': 0.31242338218575777, 'learning_rate': 0.009306346334746435, 'weight_decay': 3.358049173318259e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 02:29:11,755] Trial 39 finished with value: 1.3920473868737293 and parameters: {'hidden_size': 453, 'dropout': 0.2583776435124376, 'learning_rate': 0.0002505860996226516, 'weight_decay': 7.002029634047581e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 02:36:00,821] Trial 40 finished with value: 1.4137555239971074 and parameters: {'hidden_size': 218, 'dropout': 0.21367530512133023, 'learning_rate': 0.0001427106108823811, 'weight_decay': 7.37559975536793e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 02:49:16,946] Trial 41 finished with value: 1.261903355422728 and parameters: {'hidden_size': 349, 'dropout': 0.3225831104140998, 'learning_rate': 0.00041586759967466176, 'weight_decay': 1.9255567115059455e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 03:03:59,401] Trial 42 finished with value: 1.3187062183802933 and parameters: {'hidden_size': 392, 'dropout': 0.36465954265380257, 'learning_rate': 0.00037121332090870885, 'weight_decay': 1.4870008495012522e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 03:19:20,458] Trial 43 finished with value: 1.3289466678076394 and parameters: {'hidden_size': 402, 'dropout': 0.3658789086881166, 'learning_rate': 0.0006957000046275352, 'weight_decay': 2.6521369751136786e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 03:29:10,012] Trial 44 finished with value: 1.321812079639719 and parameters: {'hidden_size': 296, 'dropout': 0.29273376188803335, 'learning_rate': 0.00046776192919060973, 'weight_decay': 1.1682795339198051e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 03:43:07,841] Trial 45 finished with value: 1.3567763275603104 and parameters: {'hidden_size': 375, 'dropout': 0.337402971476798, 'learning_rate': 0.0020772807591206144, 'weight_decay': 1.5445614624735244e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 04:03:11,278] Trial 46 finished with value: 2.8648619529281962 and parameters: {'hidden_size': 473, 'dropout': 0.18259655907079123, 'learning_rate': 0.0009763869608333691, 'weight_decay': 4.586976992176073e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 04:18:45,597] Trial 47 finished with value: 1.2158096700830983 and parameters: {'hidden_size': 404, 'dropout': 0.48242513216182703, 'learning_rate': 0.0003052938593739495, 'weight_decay': 2.166093795510628e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 04:28:09,551] Trial 48 finished with value: 1.1502202582803287 and parameters: {'hidden_size': 274, 'dropout': 0.4945340206702563, 'learning_rate': 0.0003079644372180521, 'weight_decay': 2.201519817682633e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 04:36:49,830] Trial 49 finished with value: 1.2319097618773935 and parameters: {'hidden_size': 265, 'dropout': 0.12295355744311892, 'learning_rate': 0.000490180711722974, 'weight_decay': 1.2136144612825782e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 04:43:30,980] Trial 50 finished with value: 1.380485304281047 and parameters: {'hidden_size': 217, 'dropout': 0.441677797307589, 'learning_rate': 0.00015219029318168988, 'weight_decay': 4.902867494774783e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 04:59:39,661] Trial 51 finished with value: 1.7510233364657815 and parameters: {'hidden_size': 418, 'dropout': 0.49166908881276383, 'learning_rate': 0.0003169360339278325, 'weight_decay': 2.4680278305992622e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 05:10:33,841] Trial 52 finished with value: 1.3960905238655288 and parameters: {'hidden_size': 309, 'dropout': 0.4866347055191302, 'learning_rate': 0.0002622905398293453, 'weight_decay': 2.0923532619115224e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 05:22:38,006] Trial 53 finished with value: 1.6777779815500564 and parameters: {'hidden_size': 338, 'dropout': 0.45409433389287956, 'learning_rate': 0.00016381515944061264, 'weight_decay': 3.199161609948116e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 05:30:25,963] Trial 54 finished with value: 1.3314066529314943 and parameters: {'hidden_size': 241, 'dropout': 0.41807421625982044, 'learning_rate': 0.00022119536868029727, 'weight_decay': 6.2271826994985275e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 05:40:01,968] Trial 55 finished with value: 1.3223265847053625 and parameters: {'hidden_size': 283, 'dropout': 0.4667966970923503, 'learning_rate': 0.0003540746339929561, 'weight_decay': 0.00020538614756887265}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 05:43:25,085] Trial 56 finished with value: 1.3303418512779486 and parameters: {'hidden_size': 111, 'dropout': 0.20537376397545049, 'learning_rate': 0.00039089650305699976, 'weight_decay': 1.0033974343019895e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 05:48:40,493] Trial 57 finished with value: 1.455707084802486 and parameters: {'hidden_size': 170, 'dropout': 0.4743946076802414, 'learning_rate': 0.0003349549501872126, 'weight_decay': 1.8005105764306175e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 05:59:58,765] Trial 58 finished with value: 1.4071418239489901 and parameters: {'hidden_size': 317, 'dropout': 0.17826608646791003, 'learning_rate': 0.0001897518291034035, 'weight_decay': 8.4475149435852e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 06:19:16,652] Trial 59 finished with value: 2.992252331606172 and parameters: {'hidden_size': 462, 'dropout': 0.499596999849549, 'learning_rate': 0.0004412834745592446, 'weight_decay': 5.5052365377444955e-06}. Best is trial 37 with value: 1.143294748031732.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Validation Loss: 1.1433\n",
      "  Params: {'hidden_size': 401, 'dropout': 0.31159803520368773, 'learning_rate': 0.0004282581786099833, 'weight_decay': 3.0769712937166838e-06}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Your dataset should be defined before running Optuna\n",
    "dataset = ...  # Your dataset should be a PyTorch Dataset object\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# **Step 1: Define K-Fold Cross-Validation**\n",
    "def get_kfold_splits(dataset, k=3):\n",
    "    \"\"\"\n",
    "    Splits dataset into k folds for cross-validation.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    indices = list(range(len(dataset)))\n",
    "    return [(list(train_idx), list(val_idx)) for train_idx, val_idx in kf.split(indices)]\n",
    "\n",
    "# **Step 2: Define Optuna Objective Function**\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter tuning with K-Fold cross-validation.\n",
    "    \"\"\"\n",
    "    # **Suggest Hyperparameters**\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 64, 512)\n",
    "    num_layers = 2\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    # **Cross-Validation Setup**\n",
    "    k = 3  # Number of folds\n",
    "    path = r'C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project\\Training_data'\n",
    "    features_list = os.listdir(os.path.join(path, r'features'))\n",
    "    # Loading the dataset for this fold\n",
    "    dataset = IndependentCSVDataset(path, features_list)\n",
    "    splits = get_kfold_splits(dataset, k)\n",
    "    fold_losses = []  # Store loss for each fold\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "        # print(f\"Training Fold {fold+1}/{k}\")\n",
    "        \n",
    "\n",
    "        # **Subset datasets for this fold**\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        feature_min, feature_max = compute_global_minmax(train_subset)\n",
    "\n",
    "        global_transform = GlobalMinMaxNormalize(feature_min, feature_max)\n",
    "        train_subset.dataset.transform = global_transform\n",
    "        val_subset.dataset.transform = global_transform  # Same transform to prevent data leakage\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "        # **Create LSTM Model**\n",
    "        model = LSTMModel(input_size=600, hidden_size=hidden_size, num_layers=num_layers, output_size=100, dropout=dropout)\n",
    "        model.to(device)\n",
    "\n",
    "        # **Loss & Optimizer**\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=compute_class_weights(train_loader))\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "        )\n",
    "\n",
    "        # **Training Loop**\n",
    "        num_epochs = 10\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets.float())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # **Validation Loss for Fold**\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                outputs = model(inputs).squeeze()\n",
    "                loss = criterion(outputs, targets.float())\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                total_samples += inputs.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / total_samples\n",
    "        fold_losses.append(avg_val_loss)\n",
    "\n",
    "    # **Return the mean validation loss across all folds**\n",
    "    mean_loss = sum(fold_losses) / len(fold_losses)\n",
    "    return mean_loss\n",
    "\n",
    "# **Step 3: Run Optuna Optimization**\n",
    "study = optuna.create_study(direction=\"minimize\")  # Minimize validation loss\n",
    "study.optimize(objective, n_trials=60)\n",
    "\n",
    "# **Step 4: Best Hyperparameters**\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Validation Loss: {study.best_trial.value:.4f}\")\n",
    "print(\"  Params:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian optimization\n",
    "import BayesianOptimization as bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 8.1832\n",
      "F1 Score: 0.0170\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate_test_metrics(model, test_loader, criterion, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluates the test loss, F1-score, and ROC-AUC of a trained model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained PyTorch model.\n",
    "    - test_loader: DataLoader for the test dataset.\n",
    "    - criterion: Loss function (e.g., nn.BCEWithLogitsLoss).\n",
    "    - device: The device ('cuda' or 'cpu').\n",
    "    - threshold: Decision threshold for binary classification (default=0.5).\n",
    "\n",
    "    Returns:\n",
    "    - Average test loss\n",
    "    - F1-score\n",
    "    - ROC-AUC score\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    total_samples = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to device\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)  # (batch_size, 6000) -> raw logits\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)  # BCEWithLogitsLoss expects raw logits\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            # Convert logits to probabilities using Sigmoid\n",
    "            probs = torch.sigmoid(outputs)  # (batch_size, 6000) -> probabilities in [0,1]\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            preds = (probs > threshold).int()  # Convert to 0 or 1\n",
    "\n",
    "            # Flatten for metric calculations\n",
    "            all_targets.extend(targets.cpu().numpy().flatten())  # Convert to 1D list\n",
    "            all_predictions.extend(preds.cpu().numpy().flatten())  # Convert to 1D list\n",
    "\n",
    "    # Compute average loss\n",
    "    avg_test_loss = test_loss / total_samples\n",
    "\n",
    "    # Compute F1-score\n",
    "    f1 = f1_score(all_targets, all_predictions, average='macro')\n",
    "\n",
    "    return avg_test_loss, f1\n",
    "\n",
    "# Example Usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_loss, test_f1 = evaluate_test_metrics(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
