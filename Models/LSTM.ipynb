{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project')\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "%matplotlib widget\n",
    "import pandas as pd # for data manipulation\n",
    "from Models.load_data import *\n",
    "from Models.helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "\n",
    "path = r'C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project\\Training_data'\n",
    "features_list = os.listdir(os.path.join(path, r'features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global feature min (train set only): [ 0.0000000e+00 -3.5055832e+06 -9.1752766e+13  0.0000000e+00\n",
      " -8.4846081e+13  0.0000000e+00  0.0000000e+00 -3.5051655e+06\n",
      " -9.2097940e+13  0.0000000e+00 -7.3527542e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5073330e+06 -9.2103753e+13  0.0000000e+00\n",
      " -7.2064157e+13  0.0000000e+00  0.0000000e+00 -3.5088230e+06\n",
      " -9.1307901e+13  0.0000000e+00 -9.4136540e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5077160e+06 -9.0980284e+13  0.0000000e+00\n",
      " -8.5559138e+13  0.0000000e+00  0.0000000e+00 -3.5052565e+06\n",
      " -9.0181126e+13  0.0000000e+00 -9.5513295e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5047762e+06 -1.0885503e+14  0.0000000e+00\n",
      " -7.5441352e+13  0.0000000e+00  0.0000000e+00 -3.5020450e+06\n",
      " -9.0598057e+13  0.0000000e+00 -8.6843157e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4991772e+06 -9.0174390e+13  0.0000000e+00\n",
      " -8.3978229e+13  0.0000000e+00  0.0000000e+00 -3.5018470e+06\n",
      " -8.9870127e+13  0.0000000e+00 -8.4611669e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5028985e+06 -9.0597663e+13  0.0000000e+00\n",
      " -8.6264359e+13  0.0000000e+00  0.0000000e+00 -3.5025488e+06\n",
      " -9.1696948e+13  0.0000000e+00 -8.9664808e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4988648e+06 -9.0528843e+13  0.0000000e+00\n",
      " -8.3062486e+13  0.0000000e+00  0.0000000e+00 -3.5012805e+06\n",
      " -8.9832253e+13  0.0000000e+00 -9.1955292e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5035248e+06 -8.9004649e+13  0.0000000e+00\n",
      " -8.1493825e+13  0.0000000e+00  0.0000000e+00 -3.5021710e+06\n",
      " -9.1081845e+13  0.0000000e+00 -8.6154846e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5006015e+06 -9.1321591e+13  0.0000000e+00\n",
      " -9.0033017e+13  0.0000000e+00  0.0000000e+00 -3.5024682e+06\n",
      " -9.0184616e+13  0.0000000e+00 -1.0223858e+14  0.0000000e+00\n",
      "  0.0000000e+00 -3.5003740e+06 -9.0630387e+13  0.0000000e+00\n",
      " -8.8005600e+13  0.0000000e+00  0.0000000e+00 -3.5039878e+06\n",
      " -9.0935237e+13  0.0000000e+00 -8.7366094e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5020550e+06 -9.1725888e+13  0.0000000e+00\n",
      " -7.3404447e+13  0.0000000e+00  0.0000000e+00 -3.5014675e+06\n",
      " -9.1298145e+13  0.0000000e+00 -8.0960285e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5010530e+06 -1.2201623e+14  0.0000000e+00\n",
      " -1.1951048e+14  0.0000000e+00  0.0000000e+00 -3.4990885e+06\n",
      " -1.6370488e+14  0.0000000e+00 -9.2821516e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4995680e+06 -1.9567905e+14  0.0000000e+00\n",
      " -6.7574570e+13  0.0000000e+00  0.0000000e+00 -3.4990552e+06\n",
      " -2.1335032e+14  0.0000000e+00 -9.0322869e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5004065e+06 -2.3102158e+14  0.0000000e+00\n",
      " -8.1806141e+13  0.0000000e+00  0.0000000e+00 -3.5028748e+06\n",
      " -2.4869285e+14  0.0000000e+00 -1.2389310e+14  0.0000000e+00\n",
      "  0.0000000e+00 -3.5003882e+06 -2.6410641e+14  0.0000000e+00\n",
      " -8.0011105e+13  0.0000000e+00  0.0000000e+00 -3.5005998e+06\n",
      " -2.6520195e+14  0.0000000e+00 -8.2180743e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5007860e+06 -2.4514876e+14  0.0000000e+00\n",
      " -7.9387429e+13  0.0000000e+00  0.0000000e+00 -3.4988002e+06\n",
      " -2.2509558e+14  0.0000000e+00 -8.3987205e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5006750e+06 -2.0504241e+14  0.0000000e+00\n",
      " -1.0198516e+14  0.0000000e+00  0.0000000e+00 -3.5004390e+06\n",
      " -1.8106289e+14  0.0000000e+00 -8.6020377e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5043540e+06 -1.3230246e+14  0.0000000e+00\n",
      " -1.0534280e+14  0.0000000e+00  0.0000000e+00 -3.5025135e+06\n",
      " -8.9010714e+13  0.0000000e+00 -8.0883218e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4999708e+06 -8.7944891e+13  0.0000000e+00\n",
      " -7.3415017e+13  0.0000000e+00  0.0000000e+00 -3.4987938e+06\n",
      " -9.8941249e+13  0.0000000e+00 -7.4302347e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5011475e+06 -8.8322622e+13  0.0000000e+00\n",
      " -8.0233336e+13  0.0000000e+00  0.0000000e+00 -3.5000685e+06\n",
      " -8.8663191e+13  0.0000000e+00 -8.7340324e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4986192e+06 -9.7956921e+13  0.0000000e+00\n",
      " -8.8581956e+13  0.0000000e+00  0.0000000e+00 -3.4979920e+06\n",
      " -9.7451667e+13  0.0000000e+00 -8.7389876e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4973182e+06 -9.9315003e+13  0.0000000e+00\n",
      " -8.8427849e+13  0.0000000e+00  0.0000000e+00 -3.4983555e+06\n",
      " -9.9259672e+13  0.0000000e+00 -8.0096535e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5143608e+06 -9.8910622e+13  0.0000000e+00\n",
      " -8.8570530e+13  0.0000000e+00  0.0000000e+00 -3.5066108e+06\n",
      " -9.8107379e+13  0.0000000e+00 -1.1345752e+14  0.0000000e+00\n",
      "  0.0000000e+00 -3.5052678e+06 -9.9369840e+13  0.0000000e+00\n",
      " -8.7777580e+13  0.0000000e+00  0.0000000e+00 -3.5051470e+06\n",
      " -9.8496644e+13  0.0000000e+00 -8.1563627e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5036758e+06 -9.8597543e+13  0.0000000e+00\n",
      " -9.4438060e+13  0.0000000e+00  0.0000000e+00 -3.5024040e+06\n",
      " -9.7333631e+13  0.0000000e+00 -8.6767399e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5031328e+06 -9.7453143e+13  0.0000000e+00\n",
      " -8.0460030e+13  0.0000000e+00  0.0000000e+00 -3.5008695e+06\n",
      " -9.8545642e+13  0.0000000e+00 -9.0200445e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4998732e+06 -9.8097531e+13  0.0000000e+00\n",
      " -8.3072461e+13  0.0000000e+00  0.0000000e+00 -3.4974965e+06\n",
      " -9.8486838e+13  0.0000000e+00 -9.0609650e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4959418e+06 -9.8125700e+13  0.0000000e+00\n",
      " -7.5150989e+13  0.0000000e+00  0.0000000e+00 -3.5057680e+06\n",
      " -9.7775451e+13  0.0000000e+00 -7.9434011e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4996490e+06 -9.6981209e+13  0.0000000e+00\n",
      " -6.7185725e+13  0.0000000e+00  0.0000000e+00 -3.4964010e+06\n",
      " -9.6627788e+13  0.0000000e+00 -8.7494398e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4996778e+06 -9.8240574e+13  0.0000000e+00\n",
      " -8.8102144e+13  0.0000000e+00  0.0000000e+00 -3.4984785e+06\n",
      " -9.7337767e+13  0.0000000e+00 -9.0286764e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4987935e+06 -9.6623946e+13  0.0000000e+00\n",
      " -7.8523352e+13  0.0000000e+00  0.0000000e+00 -3.5013875e+06\n",
      " -9.8193740e+13  0.0000000e+00 -9.7430008e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5031300e+06 -9.5748075e+13  0.0000000e+00\n",
      " -7.5456544e+13  0.0000000e+00  0.0000000e+00 -3.5014380e+06\n",
      " -9.5964929e+13  0.0000000e+00 -8.5951942e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5022352e+06 -9.7440905e+13  0.0000000e+00\n",
      " -8.5267281e+13  0.0000000e+00  0.0000000e+00 -3.5014412e+06\n",
      " -9.5423579e+13  0.0000000e+00 -8.9678087e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5010368e+06 -9.5210525e+13  0.0000000e+00\n",
      " -8.7429554e+13  0.0000000e+00  0.0000000e+00 -3.4995235e+06\n",
      " -9.5523940e+13  0.0000000e+00 -7.2652014e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4987650e+06 -9.7055238e+13  0.0000000e+00\n",
      " -8.6347608e+13  0.0000000e+00  0.0000000e+00 -3.4972065e+06\n",
      " -9.6888523e+13  0.0000000e+00 -1.0141521e+14  0.0000000e+00\n",
      "  0.0000000e+00 -3.4986190e+06 -9.5819974e+13  0.0000000e+00\n",
      " -8.0866206e+13  0.0000000e+00  0.0000000e+00 -3.4991182e+06\n",
      " -9.5481703e+13  0.0000000e+00 -8.5998424e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4997158e+06 -9.6277103e+13  0.0000000e+00\n",
      " -8.8216145e+13  0.0000000e+00  0.0000000e+00 -3.4996460e+06\n",
      " -9.5833287e+13  0.0000000e+00 -8.6805114e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4996090e+06 -9.5494555e+13  0.0000000e+00\n",
      " -8.2168185e+13  0.0000000e+00  0.0000000e+00 -3.5009582e+06\n",
      " -9.5508303e+13  0.0000000e+00 -8.8669952e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5016810e+06 -9.5475689e+13  0.0000000e+00\n",
      " -9.0669729e+13  0.0000000e+00  0.0000000e+00 -3.5018210e+06\n",
      " -9.3926875e+13  0.0000000e+00 -8.8385092e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5017315e+06 -9.4840965e+13  0.0000000e+00\n",
      " -8.1228921e+13  0.0000000e+00  0.0000000e+00 -3.4992612e+06\n",
      " -9.4742365e+13  0.0000000e+00 -8.3164014e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4972810e+06 -9.4345324e+13  0.0000000e+00\n",
      " -7.0932920e+13  0.0000000e+00  0.0000000e+00 -3.4986030e+06\n",
      " -9.4344149e+13  0.0000000e+00 -9.7675668e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5135998e+06 -9.5121840e+13  0.0000000e+00\n",
      " -8.4269205e+13  0.0000000e+00  0.0000000e+00 -3.5081918e+06\n",
      " -9.4699222e+13  0.0000000e+00 -8.6575635e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5047968e+06 -9.3594644e+13  0.0000000e+00\n",
      " -9.1737482e+13  0.0000000e+00  0.0000000e+00 -3.5035562e+06\n",
      " -9.4032781e+13  0.0000000e+00 -9.0649513e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5026908e+06 -9.5150882e+13  0.0000000e+00\n",
      " -8.3268922e+13  0.0000000e+00  0.0000000e+00 -3.5017258e+06\n",
      " -9.5491946e+13  0.0000000e+00 -8.6722352e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4999545e+06 -9.4684634e+13  0.0000000e+00\n",
      " -9.1812719e+13  0.0000000e+00  0.0000000e+00 -3.4986178e+06\n",
      " -9.1995884e+13  0.0000000e+00 -8.0086779e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4986150e+06 -9.3349009e+13  0.0000000e+00\n",
      " -7.1905059e+13  0.0000000e+00  0.0000000e+00 -3.4955745e+06\n",
      " -9.3544597e+13  0.0000000e+00 -7.9882273e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4962940e+06 -9.3227089e+13  0.0000000e+00\n",
      " -8.5952219e+13  0.0000000e+00  0.0000000e+00 -3.4967928e+06\n",
      " -9.3201990e+13  0.0000000e+00 -9.5590269e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4974350e+06 -9.3962593e+13  0.0000000e+00\n",
      " -8.7464971e+13  0.0000000e+00  0.0000000e+00 -3.4979028e+06\n",
      " -9.2747562e+13  0.0000000e+00 -9.2023642e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.4988988e+06 -9.2858107e+13  0.0000000e+00\n",
      " -8.0800347e+13  0.0000000e+00  0.0000000e+00 -3.4974748e+06\n",
      " -9.2415516e+13  0.0000000e+00 -8.8420131e+13  0.0000000e+00\n",
      "  0.0000000e+00 -3.5154782e+06 -9.2070149e+13  0.0000000e+00\n",
      " -9.2316656e+13  0.0000000e+00  0.0000000e+00 -3.5084712e+06\n",
      " -9.1304906e+13  0.0000000e+00 -8.5059873e+13  0.0000000e+00]\n",
      "Global feature max (train set only): [7.5028229e+00 6.2653696e+03 6.5458751e+17 7.9735429e+17 2.2450910e+18\n",
      " 2.3698273e+02 7.5037384e+00 7.0496821e+03 6.1534924e+17 7.9877060e+17\n",
      " 1.2322810e+18 2.4030396e+02 7.5353251e+00 6.0021616e+03 5.9603742e+17\n",
      " 8.0080758e+17 4.0014726e+17 2.4072734e+02 7.5318155e+00 5.3887031e+03\n",
      " 6.5520111e+17 8.0231068e+17 4.6291127e+17 2.5956421e+02 7.5292211e+00\n",
      " 5.1789370e+03 6.5481546e+17 7.9893614e+17 4.9665332e+17 2.6361975e+02\n",
      " 7.5542469e+00 5.5654751e+03 6.5458751e+17 7.9678859e+17 4.9665332e+17\n",
      " 5.1636298e+02 7.5493641e+00 5.5346040e+03 6.5458751e+17 7.9318102e+17\n",
      " 4.9665332e+17 4.5296130e+02 7.5818658e+00 5.6381431e+03 6.5458751e+17\n",
      " 7.8922154e+17 4.9665332e+17 3.1719446e+02 7.5827813e+00 4.9983154e+03\n",
      " 6.5458751e+17 7.8526200e+17 4.9665332e+17 2.8778918e+02 7.5919371e+00\n",
      " 5.3743091e+03 6.5458751e+17 7.8130252e+17 4.9665332e+17 2.9527701e+02\n",
      " 7.6006346e+00 5.8492104e+03 6.5458751e+17 7.8181118e+17 4.9665332e+17\n",
      " 2.5331425e+02 7.5957522e+00 5.7722812e+03 6.5458751e+17 7.8424048e+17\n",
      " 4.9665332e+17 2.5532495e+02 7.9549546e+00 7.2445923e+03 6.5458751e+17\n",
      " 7.8666978e+17 4.9665332e+17 4.4181900e+02 7.9738760e+00 4.8079087e+03\n",
      " 6.5458751e+17 7.8909909e+17 4.9665332e+17 4.3258957e+02 7.9120760e+00\n",
      " 5.6968301e+03 6.5458751e+17 7.9152846e+17 4.9665332e+17 3.8798080e+02\n",
      " 7.8362379e+00 5.3632007e+03 6.5467273e+17 7.9397487e+17 4.9665332e+17\n",
      " 3.2029855e+02 7.7787104e+00 4.9513560e+03 6.5495214e+17 7.9867549e+17\n",
      " 4.9665332e+17 2.8799564e+02 7.7443767e+00 5.1938809e+03 6.5533381e+17\n",
      " 8.0353664e+17 4.9665332e+17 2.7672519e+02 7.7550588e+00 5.6177461e+03\n",
      " 6.6499116e+17 8.0663004e+17 5.1664687e+17 2.8401221e+02 7.6480908e+00\n",
      " 5.3516523e+03 6.6518509e+17 8.0952945e+17 5.1131555e+17 3.7183932e+02\n",
      " 7.6761684e+00 6.6196689e+03 6.6544650e+17 8.1242887e+17 5.2705001e+17\n",
      " 3.5334946e+02 7.6734219e+00 5.4221460e+03 6.5883218e+17 8.1532821e+17\n",
      " 4.9674303e+17 2.5074829e+02 7.6911221e+00 5.2035127e+03 6.5904486e+17\n",
      " 8.1822762e+17 4.9691700e+17 3.8601617e+02 7.7098908e+00 5.5986860e+03\n",
      " 5.7468916e+17 8.2078790e+17 4.7185892e+17 4.0046399e+02 7.6940217e+00\n",
      " 5.1457095e+03 6.0631284e+17 8.2026089e+17 4.7028995e+17 2.9831079e+02\n",
      " 7.7098908e+00 6.2590806e+03 6.1348907e+17 8.1939503e+17 4.7032503e+17\n",
      " 2.3346985e+02 7.7030244e+00 5.6124585e+03 6.3185786e+17 8.1852916e+17\n",
      " 4.7036946e+17 2.2393640e+02 7.7353740e+00 5.9632222e+03 6.1233699e+17\n",
      " 8.1766337e+17 4.7010097e+17 2.2525632e+02 7.7269816e+00 5.8130479e+03\n",
      " 4.9839014e+17 8.1679750e+17 4.7010097e+17 2.2525632e+02 7.7478867e+00\n",
      " 5.9416104e+03 6.5348498e+17 8.1614879e+17 7.8739629e+17 2.3528183e+02\n",
      " 7.7474289e+00 6.0081636e+03 6.5319773e+17 8.1748841e+17 7.8732379e+17\n",
      " 2.6404922e+02 7.7745905e+00 5.4239341e+03 6.5319773e+17 8.1904635e+17\n",
      " 1.7011927e+18 2.2937286e+02 7.7738276e+00 6.0165850e+03 6.5319773e+17\n",
      " 8.2060429e+17 1.8753850e+18 2.6636292e+02 7.7929015e+00 5.6894253e+03\n",
      " 6.5323395e+17 8.2216222e+17 9.5215487e+17 3.7343405e+02 7.7755055e+00\n",
      " 5.3745273e+03 6.5362097e+17 8.2372016e+17 6.1209338e+17 3.8725525e+02\n",
      " 7.7634511e+00 5.4494731e+03 6.5280081e+17 8.2554398e+17 4.7121804e+17\n",
      " 4.2331500e+02 7.7838984e+00 6.0167583e+03 6.2584896e+17 8.2981613e+17\n",
      " 1.8393942e+18 4.0901456e+02 7.7758107e+00 5.2687285e+03 6.4304292e+17\n",
      " 8.3435711e+17 1.5315800e+18 3.5606204e+02 7.7860346e+00 5.5470669e+03\n",
      " 4.8516143e+17 8.3889803e+17 4.7010097e+17 4.2691904e+02 7.7825251e+00\n",
      " 5.1274985e+03 4.1326426e+17 8.4343901e+17 4.7015932e+17 4.2682309e+02\n",
      " 7.7938166e+00 5.7004175e+03 3.6615809e+17 8.4797999e+17 4.7026738e+17\n",
      " 4.7320395e+02 7.7982421e+00 5.4304453e+03 3.2216519e+17 8.5139590e+17\n",
      " 4.7081009e+17 4.8836819e+02 7.8232675e+00 5.4576699e+03 3.4336669e+17\n",
      " 8.4439091e+17 4.7153003e+17 3.7316583e+02 7.8179264e+00 5.9469253e+03\n",
      " 3.7681500e+17 8.3624195e+17 4.7156205e+17 3.7428888e+02 7.8434095e+00\n",
      " 5.3573848e+03 3.6156722e+17 8.2809299e+17 4.7159164e+17 4.1135327e+02\n",
      " 7.8278451e+00 5.3027764e+03 3.4282965e+17 8.1994403e+17 4.7162266e+17\n",
      " 4.1457623e+02 7.8458509e+00 4.9140137e+03 3.1290486e+17 8.1179507e+17\n",
      " 4.3968182e+17 3.7316583e+02 7.8264718e+00 5.1672129e+03 3.1916826e+17\n",
      " 8.0396799e+17 4.5759887e+17 3.1114746e+02 7.8423414e+00 5.6106416e+03\n",
      " 3.4712228e+17 7.9914017e+17 4.7014080e+17 2.3335298e+02 7.8492084e+00\n",
      " 6.7116152e+03 3.3565118e+17 7.9464165e+17 4.7014080e+17 2.3936926e+02\n",
      " 7.8553119e+00 5.4208560e+03 2.8326400e+17 7.9014307e+17 4.7014080e+17\n",
      " 2.5720020e+02 7.7405624e+00 5.2867065e+03 2.7739410e+17 7.8564456e+17\n",
      " 4.7014080e+17 3.8922922e+02 7.7466660e+00 5.3259346e+03 3.3062119e+17\n",
      " 7.8114604e+17 4.6336055e+17 3.8401514e+02 7.7657399e+00 5.0096611e+03\n",
      " 3.8402491e+17 7.7705119e+17 4.4075978e+17 3.2798047e+02 7.7565842e+00\n",
      " 4.7438945e+03 4.5390493e+17 7.7658362e+17 4.4281690e+17 2.9409357e+02\n",
      " 6.9882812e+00 5.6239692e+03 5.2561569e+17 7.7651408e+17 4.7464120e+17\n",
      " 2.8900284e+02 7.2806482e+00 6.0356470e+03 5.5243065e+17 7.7644446e+17\n",
      " 4.2972041e+17 2.6439758e+02 7.0162053e+00 5.1904678e+03 5.7425712e+17\n",
      " 7.7950612e+17 4.2043249e+17 2.3952510e+02 6.9989624e+00 5.4257910e+03\n",
      " 5.7562052e+17 7.8383380e+17 4.3705725e+17 2.3983968e+02 7.0300913e+00\n",
      " 5.0721909e+03 5.7471019e+17 7.8816148e+17 4.4834253e+17 2.4306415e+02\n",
      " 7.0531325e+00 5.7645864e+03 5.6773637e+17 7.9248922e+17 4.4576833e+17\n",
      " 2.4668184e+02 7.0763268e+00 5.5299800e+03 5.6008882e+17 7.9616874e+17\n",
      " 4.4146732e+17 2.4738966e+02 7.0720544e+00 5.5345845e+03 5.5844618e+17\n",
      " 7.9391522e+17 4.6732519e+17 2.4854752e+02 7.0769372e+00 5.5724189e+03\n",
      " 5.5747074e+17 7.9101045e+17 4.7031194e+17 2.7410007e+02 7.1036410e+00\n",
      " 6.3569229e+03 5.8261857e+17 7.8810568e+17 4.7034960e+17 2.9094150e+02\n",
      " 7.0920439e+00 6.1147422e+03 6.1066896e+17 7.8520084e+17 4.7039440e+17\n",
      " 2.9442593e+02 7.1324811e+00 4.9786279e+03 6.3269204e+17 7.8229606e+17\n",
      " 4.7836387e+17 2.8776566e+02 7.1282082e+00 6.5067720e+03 6.5404539e+17\n",
      " 7.7923853e+17 4.8583285e+17 2.8406863e+02 7.1456037e+00 5.1272261e+03\n",
      " 5.1466229e+17 7.7477485e+17 4.4735349e+17 2.6619962e+02 7.1478925e+00\n",
      " 5.7590005e+03 3.5741931e+17 7.7015684e+17 4.0313515e+17 2.3208229e+02\n",
      " 7.1707816e+00 6.3689644e+03 2.7295223e+17 7.6553882e+17 4.0313515e+17\n",
      " 2.3208229e+02 7.1762753e+00 6.2576313e+03 3.5286087e+17 7.6424208e+17\n",
      " 4.0313515e+17 2.3208229e+02 7.1851254e+00 5.4551680e+03 3.8802459e+17\n",
      " 7.6658301e+17 4.0313515e+17 2.3546515e+02 7.1742911e+00 6.2909043e+03\n",
      " 6.5645833e+17 7.6892387e+17 4.0313515e+17 2.3664850e+02 7.1868038e+00\n",
      " 5.0492661e+03 6.4479629e+17 7.7126480e+17 4.0313515e+17 2.3266083e+02\n",
      " 7.1985536e+00 4.9525303e+03 5.8328618e+17 7.7388102e+17 4.0313515e+17\n",
      " 2.4578050e+02 7.2290721e+00 5.1851108e+03 5.1751906e+17 7.7899499e+17\n",
      " 4.0313515e+17 2.8818234e+02 7.2249522e+00 4.8665244e+03 5.2561153e+17\n",
      " 7.8438314e+17 4.0313515e+17 2.7735864e+02 7.2354808e+00 4.8622422e+03\n",
      " 5.8199617e+17 7.8977123e+17 4.0313515e+17 2.8241064e+02 7.2681360e+00\n",
      " 5.7215479e+03 6.3383705e+17 7.9515939e+17 4.0313515e+17 3.7597409e+02\n",
      " 7.2739344e+00 6.2085054e+03 6.5608147e+17 8.0054748e+17 4.0313515e+17\n",
      " 4.5006165e+02 7.2801905e+00 5.2589912e+03 6.5625664e+17 8.0517511e+17\n",
      " 4.0313515e+17 4.9961023e+02 7.2788172e+00 5.4839399e+03 6.5650431e+17\n",
      " 8.0286456e+17 3.7006844e+17 5.2325842e+02 7.3096404e+00 5.3295024e+03\n",
      " 6.5663824e+17 7.9979245e+17 3.5856974e+17 2.5733704e+02 7.3078094e+00\n",
      " 5.7007871e+03 6.5670284e+17 7.9672035e+17 3.9717975e+17 4.0318082e+02\n",
      " 7.3357339e+00 5.5564985e+03 6.5682976e+17 7.9364825e+17 3.8625572e+17\n",
      " 2.5468297e+02 7.3294778e+00 5.7973633e+03 6.1849515e+17 7.9057614e+17\n",
      " 4.5661526e+17 2.4150473e+02 7.3363447e+00 4.9906235e+03 4.2748469e+17\n",
      " 7.8775466e+17 4.7092598e+17 2.2493408e+02 7.3668628e+00 5.7527578e+03\n",
      " 2.7953556e+17 7.8723260e+17 4.7092598e+17 2.3368826e+02 7.3645740e+00\n",
      " 5.7717168e+03 2.6995374e+17 7.8696294e+17 4.7092598e+17 2.3561514e+02\n",
      " 7.3570971e+00 6.0715693e+03 2.5448580e+17 7.8669322e+17 4.7092598e+17\n",
      " 2.4323296e+02 7.3833427e+00 5.9026904e+03 2.3145378e+17 7.8642356e+17\n",
      " 4.7092598e+17 2.5983231e+02 7.3894467e+00 6.8746826e+03 2.0867011e+17\n",
      " 7.8615384e+17 4.7092598e+17 2.6627808e+02 7.3981447e+00 4.6042095e+03\n",
      " 6.2548481e+17 7.8607880e+17 1.4398046e+18 2.9024918e+02 7.4176764e+00\n",
      " 5.8378110e+03 4.1406161e+17 7.8780008e+17 5.6515609e+17 3.3635941e+02\n",
      " 7.4164557e+00 4.8385967e+03 1.2220701e+17 7.8971859e+17 4.0539296e+17\n",
      " 3.6648914e+02 7.4390392e+00 5.9464858e+03 1.5315690e+17 7.9163703e+17\n",
      " 3.5065800e+17 3.7273550e+02 7.4515519e+00 5.0224058e+03 2.7612685e+17\n",
      " 7.9355554e+17 4.7101717e+17 3.3189801e+02 7.4701681e+00 4.8503345e+03\n",
      " 4.9782942e+17 7.9547405e+17 1.0108969e+18 2.4626016e+02 7.4694052e+00\n",
      " 5.7776182e+03 6.5544025e+17 7.9721905e+17 1.8856209e+18 2.2111931e+02]\n"
     ]
    }
   ],
   "source": [
    "# This cell takes 19s\n",
    "# Instantiate the dataset without transformations\n",
    "dataset = IndependentCSVDataset(path, features_list)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "feature_min, feature_max = compute_global_minmax(train_dataset)\n",
    "print(\"Global feature min (train set only):\", feature_min)\n",
    "print(\"Global feature max (train set only):\", feature_max)\n",
    "\n",
    "global_transform = GlobalMinMaxNormalize(feature_min, feature_max)\n",
    "train_dataset.dataset.transform = global_transform\n",
    "test_dataset.dataset.transform = global_transform  # Same transform to prevent data leakage\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([32, 60, 600])\n",
      "Targets shape: torch.Size([32, 60, 100])\n"
     ]
    }
   ],
   "source": [
    "# Create an iterator from the DataLoader\n",
    "train_iterator = iter(train_loader)\n",
    "\n",
    "# Get a single batch\n",
    "batch = next(train_iterator)\n",
    "\n",
    "# Unpack the batch (assuming your dataset returns (inputs, targets))\n",
    "inputs, targets = batch\n",
    "\n",
    "print(\"Inputs shape:\", inputs.shape)\n",
    "print(\"Targets shape:\", targets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm): LSTM(600, 401, num_layers=2, batch_first=True, dropout=0.31159803520368773)\n",
      "  (fc1): Linear(in_features=401, out_features=401, bias=True)\n",
      "  (fc2): Linear(in_features=401, out_features=100, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Input shape: torch.Size([32, 60, 600])\n",
      "Output shape: torch.Size([32, 60, 100])\n",
      "Targets shape: torch.Size([32, 60, 100])\n",
      "1.4426698684692383\n"
     ]
    }
   ],
   "source": [
    "# Define loss and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=compute_class_weights(train_loader))\n",
    "# Define the model and print the architecture, example with dummy input shapes\n",
    "# [I 2025-02-07 01:30:51,832] Trial 37 finished with value: 1.143294748031732 and parameters: \n",
    "# {'hidden_size': 401, 'dropout': 0.31159803520368773, 'learning_rate': 0.0004282581786099833, \n",
    "# 'weight_decay': 3.0769712937166838e-06}. Best is trial 37 with value: 1.143294748031732.\n",
    "\n",
    "input_size = 600       # Number of features per time step\n",
    "hidden_size = 401     # Number of LSTM hidden units\n",
    "num_layers = 2       # Number of LSTM layers\n",
    "output_size = 100      # Binary classification\n",
    "dropout = 0.31159803520368773\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "\n",
    "# Print model architecture\n",
    "print(model)\n",
    "\n",
    "# Example dummy input: batch_size=32, sequence_length=6000, input_size=6\n",
    "dummy_input = inputs\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "print(\"Input shape:\", dummy_input.shape)  # Expected: (32, 6000, 6) -> 6 features per timestep\n",
    "print(\"Output shape:\", dummy_output.shape)  # Expected: (32, 6000,) -> one binary value per timestep\n",
    "print(\"Targets shape:\", targets.shape)\n",
    "\n",
    "loss = criterion(dummy_output, targets)\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.3724.9794, batch_idx=7\n",
      "Epoch [2/20], Loss: 1.5791.6327, batch_idx=7\n",
      "Epoch [3/20], Loss: 1.3201.5605, batch_idx=7\n",
      "Epoch [4/20], Loss: 1.3248.5987, batch_idx=7\n",
      "Epoch [5/20], Loss: 1.3475.7803, batch_idx=7\n",
      "Epoch [6/20], Loss: 1.4990.9918, batch_idx=7\n",
      "Epoch [7/20], Loss: 1.4537.6298, batch_idx=7\n",
      "Epoch [8/20], Loss: 1.4650.7198, batch_idx=7\n",
      "Epoch [9/20], Loss: 1.2846.2771, batch_idx=7\n",
      "Epoch [10/20], Loss: 1.2258067, batch_idx=7\n",
      "Epoch [11/20], Loss: 1.29533625, batch_idx=7\n",
      "Epoch [12/20], Loss: 1.1767135, batch_idx=7\n",
      "Epoch [13/20], Loss: 1.2092738, batch_idx=7\n",
      "Epoch [14/20], Loss: 1.0732852, batch_idx=7\n",
      "Epoch [15/20], Loss: 1.2329635, batch_idx=7\n",
      "Epoch [16/20], Loss: 1.47968369, batch_idx=7\n",
      "Epoch [17/20], Loss: 1.41873496, batch_idx=7\n",
      "Epoch [18/20], Loss: 1.37950361, batch_idx=7\n",
      "Epoch [19/20], Loss: 1.29503598, batch_idx=7\n",
      "Epoch [20/20], Loss: 1.34857877, batch_idx=7\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0004282581786099833, weight_decay=3.0769712937166838e-06)\n",
    "\n",
    "# Assume `train_loader` is a DataLoader yielding batches of (input, target)\n",
    "num_epochs = 20\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        print(f'Accumulated epoch loss: {epoch_loss:.4f}, {batch_idx=}', end='\\r')\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), r'C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project\\Models\\lstm_model.pth')\n",
    "print('Model saved at C:\\\\Users\\\\Max Tost\\\\Desktop\\\\Notebooks\\\\SPC Neural Network Project\\\\Models\\\\lstm_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting outputs of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(600, 401, num_layers=2, batch_first=True, dropout=0.31159803520368773)\n",
       "  (fc1): Linear(in_features=401, out_features=401, bias=True)\n",
       "  (fc2): Linear(in_features=401, out_features=100, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters:\n",
    "input_size = 600       # Number of features per time step\n",
    "hidden_size = 401     # Number of LSTM hidden units\n",
    "num_layers = 2       # Number of LSTM layers\n",
    "output_size = 100      # Binary classification\n",
    "dropout = 0.31159803520368773\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "model.load_state_dict(torch.load(r'C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project\\Models\\lstm_model.pth', weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([60, 600])\n",
      "Targets shape: torch.Size([60, 100])\n",
      "Outputs shape: torch.Size([60, 100])\n"
     ]
    }
   ],
   "source": [
    "# Create an iterator from the DataLoader\n",
    "train_iterator = iter(train_loader)\n",
    "\n",
    "# Get a single batch\n",
    "batch = next(train_iterator)\n",
    "\n",
    "# Unpack the batch (assuming your dataset returns (inputs, targets))\n",
    "inputs, targets = batch\n",
    "input, target = inputs[0], targets[0]\n",
    "\n",
    "print(\"Inputs shape:\", input.shape)\n",
    "print(\"Targets shape:\", target.shape)\n",
    "\n",
    "output = model(input)\n",
    "\n",
    "print(\"Outputs shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-39632.2656, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([60, 600])\n",
      "Targets shape: torch.Size([60, 100])\n",
      "Outputs shape: torch.Size([60, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23e59f554d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a5212d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2d650>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2b350>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f292d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2a090>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2a710>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f29f50>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f290d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2a3d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f6a7d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a50d3d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2a2d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f29a10>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2ac10>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2bf50>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2bb10>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fdb750>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ef0c10>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fd8990>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fd9f90>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2a690>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fd9e90>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fd9990>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a5235d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ebb050>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f6a810>,\n",
       " <matplotlib.lines.Line2D at 0x23e59feab50>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fea350>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ff27d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4b3550>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fe8990>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4d3010>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a520190>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fd8390>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ef03d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ff8f50>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ff9350>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ff96d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ff9a90>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ff9e90>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a522150>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffa4d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fea810>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffac10>,\n",
       " <matplotlib.lines.Line2D at 0x23e59feaf90>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffb3d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffb7d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffbbd0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4a3f90>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f28390>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a520750>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4e8ad0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fd8c90>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4e9210>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4e9610>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4e99d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fd9d50>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f6a890>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffa450>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffa810>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffab90>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4eafd0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4eb390>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4eb710>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4eb890>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4ebe10>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4e8110>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4e84d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a508710>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a508cd0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a509090>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a509490>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a509850>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f29b90>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a509fd0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f2a310>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f6ae90>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a50ab10>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a50ae90>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a50b010>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ddbdd0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a50b9d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a50bdd0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fd80d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4c0550>,\n",
       " <matplotlib.lines.Line2D at 0x23e43190090>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4c0cd0>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4c1050>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4c1410>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4b2010>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a521250>,\n",
       " <matplotlib.lines.Line2D at 0x23e59f21e90>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4b2b10>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4c2650>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4c29d0>,\n",
       " <matplotlib.lines.Line2D at 0x23e59ffad90>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4c3110>,\n",
       " <matplotlib.lines.Line2D at 0x23e59fdb450>,\n",
       " <matplotlib.lines.Line2D at 0x23e5a4c3850>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4906147fa1a401f85b7dcbaac8befdf",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAYAAAA10dzkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAhaElEQVR4nO3df5DU9X348dfpwZ0ksBoJd1xFPI2jUNTiUfFILphGTzH+amyDWi9pm9Jco0GgmSCSjoyZcEpTap1TjATbOLHqt6NY+h0kXEa9kHIo0EMpUutUIo5yIVi8vWp6KH6+f/hlk8sdv/zB7uX9eMzsjPve92d5f95zsk8/e7uWZVmWBQAAyTiq2AsAAODIEoAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkpL/YCBrN33nknXn311Rg+fHiUlZUVezkAwCHIsix6enqipqYmjjoqzWthAvB9ePXVV2PMmDHFXgYA8B68/PLLccIJJxR7GUUhAN+H4cOHR8S7P0AjRowo8moAgEORz+djzJgxhdfxFAnA92Hf274jRowQgAAwyKT861tpvvENAJAwAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJCYQROAd911V9TW1kZlZWXU1dXFmjVrDji/vb096urqorKyMk4++eS4++679zv3wQcfjLKysrjiiis+4FUDAJSeQRGADz30UMyaNSvmz58fnZ2d0dDQENOmTYvt27cPOH/btm1x8cUXR0NDQ3R2dsZNN90UM2fOjIcffrjf3Jdeeim+/vWvR0NDw4d9GgAAJaEsy7Ks2Is4mMmTJ8fZZ58dS5YsKYyNGzcurrjiimhpaek3f+7cubFixYrYunVrYay5uTmeeeaZ6OjoKIzt3bs3pk6dGn/yJ38Sa9asiddffz0effTRQ15XPp+PXC4X3d3dMWLEiPd2cgDAEeX1exBcAdyzZ09s3LgxGhsb+4w3NjbG2rVrBzymo6Oj3/wLL7wwNmzYEG+99VZh7JZbbomPf/zj8eUvf/mQ1tLb2xv5fL7PDQBgsCn5ANy1a1fs3bs3qqqq+oxXVVVFV1fXgMd0dXUNOP/tt9+OXbt2RUTEv/7rv8ayZcti6dKlh7yWlpaWyOVyhduYMWMO82wAAIqv5ANwn7Kysj73syzrN3aw+fvGe3p64tprr42lS5fGyJEjD3kN8+bNi+7u7sLt5ZdfPowzAAAoDeXFXsDBjBw5Mo4++uh+V/t27tzZ7yrfPtXV1QPOLy8vj+OPPz62bNkSP/3pT+PSSy8tPP7OO+9ERER5eXk8//zzccopp/R73oqKiqioqHi/pwQAUFQlfwVw6NChUVdXF21tbX3G29raYsqUKQMeU19f32/+6tWrY9KkSTFkyJA4/fTTY/PmzbFp06bC7bLLLovPfOYzsWnTJm/tAgC/0Ur+CmBExJw5c6KpqSkmTZoU9fX1cc8998T27dujubk5It59a/aVV16J++67LyLe/cRva2trzJkzJ2bMmBEdHR2xbNmyeOCBByIiorKyMiZMmNDnzzj22GMjIvqNAwD8phkUATh9+vR47bXX4pZbbokdO3bEhAkTYuXKlTF27NiIiNixY0ef7wSsra2NlStXxuzZs+POO++MmpqauOOOO+LKK68s1ikAAJSMQfE9gKXK9wgBwODj9XsQ/A4gAAAfLAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJCYQROAd911V9TW1kZlZWXU1dXFmjVrDji/vb096urqorKyMk4++eS4++67+zy+dOnSaGhoiOOOOy6OO+64OP/88+Ppp5/+ME8BAKAkDIoAfOihh2LWrFkxf/786OzsjIaGhpg2bVps3759wPnbtm2Liy++OBoaGqKzszNuuummmDlzZjz88MOFOU8++WRcffXV8cQTT0RHR0eceOKJ0djYGK+88sqROi0AgKIoy7IsK/YiDmby5Mlx9tlnx5IlSwpj48aNiyuuuCJaWlr6zZ87d26sWLEitm7dWhhrbm6OZ555Jjo6Ogb8M/bu3RvHHXdctLa2xhe/+MVDWlc+n49cLhfd3d0xYsSIwzwrAKAYvH4PgiuAe/bsiY0bN0ZjY2Of8cbGxli7du2Ax3R0dPSbf+GFF8aGDRvirbfeGvCYN998M95666342Mc+tt+19Pb2Rj6f73MDABhsSj4Ad+3aFXv37o2qqqo+41VVVdHV1TXgMV1dXQPOf/vtt2PXrl0DHnPjjTfGb/3Wb8X555+/37W0tLRELpcr3MaMGXOYZwMAUHwlH4D7lJWV9bmfZVm/sYPNH2g8ImLRokXxwAMPxCOPPBKVlZX7fc558+ZFd3d34fbyyy8fzikAAJSE8mIv4GBGjhwZRx99dL+rfTt37ux3lW+f6urqAeeXl5fH8ccf32f8O9/5TixcuDB+9KMfxZlnnnnAtVRUVERFRcV7OAsAgNJR8lcAhw4dGnV1ddHW1tZnvK2tLaZMmTLgMfX19f3mr169OiZNmhRDhgwpjP31X/91fOtb34pVq1bFpEmTPvjFAwCUoJIPwIiIOXPmxPe+97249957Y+vWrTF79uzYvn17NDc3R8S7b83+6id3m5ub46WXXoo5c+bE1q1b4957741ly5bF17/+9cKcRYsWxTe/+c24995746STToqurq7o6uqK//mf/zni5wcAcCSV/FvAERHTp0+P1157LW655ZbYsWNHTJgwIVauXBljx46NiIgdO3b0+U7A2traWLlyZcyePTvuvPPOqKmpiTvuuCOuvPLKwpy77ror9uzZE3/wB3/Q58+6+eabY8GCBUfkvAAAimFQfA9gqfI9QgAw+Hj9HiRvAQMA8MERgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRk0AXjXXXdFbW1tVFZWRl1dXaxZs+aA89vb26Ouri4qKyvj5JNPjrvvvrvfnIcffjjGjx8fFRUVMX78+Fi+fPmHtXwAgJIxKALwoYceilmzZsX8+fOjs7MzGhoaYtq0abF9+/YB52/bti0uvvjiaGhoiM7Ozrjpppti5syZ8fDDDxfmdHR0xPTp06OpqSmeeeaZaGpqii984Qvx1FNPHanTAgAoirIsy7JiL+JgJk+eHGeffXYsWbKkMDZu3Li44ooroqWlpd/8uXPnxooVK2Lr1q2Fsebm5njmmWeio6MjIiKmT58e+Xw+HnvsscKciy66KI477rh44IEHDmld+Xw+crlcdHd3x4gRI97r6fXzf3/wf6LzuSc+sOcDgMFq4vjPxCXXfuEDfc4P6/V7MCkv9gIOZs+ePbFx48a48cYb+4w3NjbG2rVrBzymo6MjGhsb+4xdeOGFsWzZsnjrrbdiyJAh0dHREbNnz+435/bbb9/vWnp7e6O3t7dwP5/PH+bZHJrO556IOy/4yofy3AAwmFzX9t24JD7YAGQQvAW8a9eu2Lt3b1RVVfUZr6qqiq6urgGP6erqGnD+22+/Hbt27TrgnP09Z0RES0tL5HK5wm3MmDHv5ZQAAIqq5K8A7lNWVtbnfpZl/cYONv/Xxw/3OefNmxdz5swp3M/n8x9KBE4c/5m4ru27H/jzAsBgM3H8Z4q9hN9IJR+AI0eOjKOPPrrflbmdO3f2u4K3T3V19YDzy8vL4/jjjz/gnP09Z0RERUVFVFRUvJfTOCyXXPsFl7sBgA9Nyb8FPHTo0Kirq4u2trY+421tbTFlypQBj6mvr+83f/Xq1TFp0qQYMmTIAefs7zkBAH5TlPwVwIiIOXPmRFNTU0yaNCnq6+vjnnvuie3bt0dzc3NEvPvW7CuvvBL33XdfRLz7id/W1taYM2dOzJgxIzo6OmLZsmV9Pt17ww03xKc//em47bbb4vLLL49//ud/jh/96Efxk5/8pCjnCABwpAyKAJw+fXq89tprccstt8SOHTtiwoQJsXLlyhg7dmxEROzYsaPPdwLW1tbGypUrY/bs2XHnnXdGTU1N3HHHHXHllVcW5kyZMiUefPDB+OY3vxl/9Vd/Faeccko89NBDMXny5CN+fgAAR9Kg+B7AUuV7hABg8PH6PQh+BxAAgA+WAAQASIwABABIjAAEAEiMAAQASIwABABIjAAEAEiMAAQASIwABABIjAAEAEiMAAQASIwABABIjAAEAEiMAAQASIwABABIjAAEAEiMAAQASIwABABIjAAEAEiMAAQASIwABABIjAAEAEiMAAQASIwABABIjAAEAEiMAAQASIwABABIjAAEAEiMAAQASIwABABIjAAEAEiMAAQASIwABABIjAAEAEiMAAQASIwABABIjAAEAEiMAAQASIwABABIjAAEAEiMAAQASIwABABIjAAEAEiMAAQASIwABABIjAAEAEiMAAQASIwABABIjAAEAEiMAAQASIwABABIjAAEAEiMAAQASIwABABIjAAEAEiMAAQASEzJB+Du3bujqakpcrlc5HK5aGpqitdff/2Ax2RZFgsWLIiampo45phj4rzzzostW7YUHv/v//7v+NrXvhannXZaDBs2LE488cSYOXNmdHd3f8hnAwBQfCUfgNdcc01s2rQpVq1aFatWrYpNmzZFU1PTAY9ZtGhRLF68OFpbW2P9+vVRXV0dF1xwQfT09ERExKuvvhqvvvpqfOc734nNmzfHP/zDP8SqVaviy1/+8pE4JQCAoirLsiwr9iL2Z+vWrTF+/PhYt25dTJ48OSIi1q1bF/X19fEf//Efcdppp/U7JsuyqKmpiVmzZsXcuXMjIqK3tzeqqqritttui6985SsD/ln/9E//FNdee2288cYbUV5efkjry+fzkcvloru7O0aMGPEezxIAOJK8fpf4FcCOjo7I5XKF+IuIOPfccyOXy8XatWsHPGbbtm3R1dUVjY2NhbGKioqYOnXqfo+JiMIPwaHGHwDAYFXStdPV1RWjRo3qNz5q1Kjo6ura7zEREVVVVX3Gq6qq4qWXXhrwmNdeey2+9a1v7ffq4D69vb3R29tbuJ/P5w84HwCgFBXlCuCCBQuirKzsgLcNGzZERERZWVm/47MsG3D8V/364/s7Jp/Px+c+97kYP3583HzzzQd8zpaWlsKHUXK5XIwZM+ZgpwoAUHKKcgXw+uuvj6uuuuqAc0466aR49tln42c/+1m/x37+85/3u8K3T3V1dUS8eyVw9OjRhfGdO3f2O6anpycuuuii+OhHPxrLly+PIUOGHHBN8+bNizlz5hTu5/N5EQgADDpFCcCRI0fGyJEjDzqvvr4+uru74+mnn45zzjknIiKeeuqp6O7ujilTpgx4TG1tbVRXV0dbW1tMnDgxIiL27NkT7e3tcdtttxXm5fP5uPDCC6OioiJWrFgRlZWVB11PRUVFVFRUHMopAgCUrJL+EMi4cePioosuihkzZsS6deti3bp1MWPGjLjkkkv6fAL49NNPj+XLl0fEu2/9zpo1KxYuXBjLly+Pf//3f48//uM/jmHDhsU111wTEe9e+WtsbIw33ngjli1bFvl8Prq6uqKrqyv27t1blHMFADhSSvpDIBER999/f8ycObPwqd7LLrssWltb+8x5/vnn+3yJ8ze+8Y34xS9+EV/96ldj9+7dMXny5Fi9enUMHz48IiI2btwYTz31VEREfOITn+jzXNu2bYuTTjrpQzwjAIDiKunvASx1vkcIAAYfr98l/hYwAAAfPAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJCYkg/A3bt3R1NTU+RyucjlctHU1BSvv/76AY/JsiwWLFgQNTU1ccwxx8R5550XW7Zs2e/cadOmRVlZWTz66KMf/AkAAJSYkg/Aa665JjZt2hSrVq2KVatWxaZNm6KpqemAxyxatCgWL14cra2tsX79+qiuro4LLrggenp6+s29/fbbo6ys7MNaPgBAySkv9gIOZOvWrbFq1apYt25dTJ48OSIili5dGvX19fH888/Haaed1u+YLMvi9ttvj/nz58fnP//5iIj4/ve/H1VVVfGP//iP8ZWvfKUw95lnnonFixfH+vXrY/To0UfmpAAAiqykrwB2dHRELpcrxF9ExLnnnhu5XC7Wrl074DHbtm2Lrq6uaGxsLIxVVFTE1KlT+xzz5ptvxtVXXx2tra1RXV394Z0EAECJKekrgF1dXTFq1Kh+46NGjYqurq79HhMRUVVV1We8qqoqXnrppcL92bNnx5QpU+Lyyy8/5PX09vZGb29v4X4+nz/kYwEASkVRrgAuWLAgysrKDnjbsGFDRMSAv5+XZdlBf2/v1x//1WNWrFgRjz/+eNx+++2Hte6WlpbCh1FyuVyMGTPmsI4HACgFRbkCeP3118dVV111wDknnXRSPPvss/Gzn/2s32M///nP+13h22ff27ldXV19fq9v586dhWMef/zx+K//+q849thj+xx75ZVXRkNDQzz55JMDPve8efNizpw5hfv5fF4EAgCDTlECcOTIkTFy5MiDzquvr4/u7u54+umn45xzzomIiKeeeiq6u7tjypQpAx5TW1sb1dXV0dbWFhMnToyIiD179kR7e3vcdtttERFx4403xp/92Z/1Oe6MM86Iv/3bv41LL710v+upqKiIioqKQzpHAIBSVdK/Azhu3Li46KKLYsaMGfHd7343IiL+/M//PC655JI+nwA+/fTTo6WlJX7/938/ysrKYtasWbFw4cI49dRT49RTT42FCxfGsGHD4pprromId68SDvTBjxNPPDFqa2uPzMkBABRJSQdgRMT9998fM2fOLHyq97LLLovW1tY+c55//vno7u4u3P/GN74Rv/jFL+KrX/1q7N69OyZPnhyrV6+O4cOHH9G1AwCUorIsy7JiL2Kwyufzkcvloru7O0aMGFHs5QAAh8Drd4l/DyAAAB88AQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJCY8mIvYDDLsiwiIvL5fJFXAgAcqn2v2/tex1MkAN+Hnp6eiIgYM2ZMkVcCAByunp6eyOVyxV5GUZRlKefv+/TOO+/Eq6++GsOHD4+ysrIP9Lnz+XyMGTMmXn755RgxYsQH+ty/iezX4bFfh8+eHR77dfjs2eF5P/uVZVn09PRETU1NHHVUmr8N5wrg+3DUUUfFCSec8KH+GSNGjPAXwWGwX4fHfh0+e3Z47Nfhs2eH573uV6pX/vZJM3sBABImAAEAEiMAS1RFRUXcfPPNUVFRUeylDAr26/DYr8Nnzw6P/Tp89uzw2K/3x4dAAAAS4wogAEBiBCAAQGIEIABAYgQgAEBiBGAJuuuuu6K2tjYqKyujrq4u1qxZU+wllYwf//jHcemll0ZNTU2UlZXFo48+2ufxLMtiwYIFUVNTE8ccc0ycd955sWXLluIstgS0tLTE7/7u78bw4cNj1KhRccUVV8Tzzz/fZ449+6UlS5bEmWeeWfhi2fr6+njssccKj9urA2tpaYmysrKYNWtWYcye9bVgwYIoKyvrc6uuri48br/6e+WVV+Laa6+N448/PoYNGxa/8zu/Exs3biw8bs/eGwFYYh566KGYNWtWzJ8/Pzo7O6OhoSGmTZsW27dvL/bSSsIbb7wRZ511VrS2tg74+KJFi2Lx4sXR2toa69evj+rq6rjgggsK/9/m1LS3t8d1110X69ati7a2tnj77bejsbEx3njjjcIce/ZLJ5xwQtx6662xYcOG2LBhQ/ze7/1eXH755YUXE3u1f+vXr4977rknzjzzzD7j9qy/3/7t344dO3YUbps3by48Zr/62r17d3zyk5+MIUOGxGOPPRbPPfdc/M3f/E0ce+yxhTn27D3KKCnnnHNO1tzc3Gfs9NNPz2688cYirah0RUS2fPnywv133nknq66uzm699dbC2P/+7/9muVwuu/vuu4uwwtKzc+fOLCKy9vb2LMvs2aE47rjjsu9973v26gB6enqyU089NWtra8umTp2a3XDDDVmW+fkayM0335ydddZZAz5mv/qbO3du9qlPfWq/j9uz984VwBKyZ8+e2LhxYzQ2NvYZb2xsjLVr1xZpVYPHtm3boqurq8/+VVRUxNSpU+3f/9fd3R0RER/72Mciwp4dyN69e+PBBx+MN954I+rr6+3VAVx33XXxuc99Ls4///w+4/ZsYC+88ELU1NREbW1tXHXVVfHiiy9GhP0ayIoVK2LSpEnxh3/4hzFq1KiYOHFiLF26tPC4PXvvBGAJ2bVrV+zduzeqqqr6jFdVVUVXV1eRVjV47Nsj+zewLMtizpw58alPfSomTJgQEfZsIJs3b46PfvSjUVFREc3NzbF8+fIYP368vdqPBx98MP7t3/4tWlpa+j1mz/qbPHly3HffffHDH/4wli5dGl1dXTFlypR47bXX7NcAXnzxxViyZEmceuqp8cMf/jCam5tj5syZcd9990WEn7H3o7zYC6C/srKyPvezLOs3xv7Zv4Fdf/318eyzz8ZPfvKTfo/Zs1867bTTYtOmTfH666/Hww8/HF/60peivb298Li9+qWXX345brjhhli9enVUVlbud549+6Vp06YV/vmMM86I+vr6OOWUU+L73/9+nHvuuRFhv37VO++8E5MmTYqFCxdGRMTEiRNjy5YtsWTJkvjiF79YmGfPDp8rgCVk5MiRcfTRR/f7r5adO3f2+68b+tv3STr719/Xvva1WLFiRTzxxBNxwgknFMbtWX9Dhw6NT3ziEzFp0qRoaWmJs846K/7u7/7OXg1g48aNsXPnzqirq4vy8vIoLy+P9vb2uOOOO6K8vLywL/Zs/z7ykY/EGWecES+88IKfsQGMHj06xo8f32ds3LhxhQ9G2rP3TgCWkKFDh0ZdXV20tbX1GW9ra4spU6YUaVWDR21tbVRXV/fZvz179kR7e3uy+5dlWVx//fXxyCOPxOOPPx61tbV9HrdnB5dlWfT29tqrAXz2s5+NzZs3x6ZNmwq3SZMmxR/90R/Fpk2b4uSTT7ZnB9Hb2xtbt26N0aNH+xkbwCc/+cl+X131n//5nzF27NiI8HfY+1KsT58wsAcffDAbMmRItmzZsuy5557LZs2alX3kIx/JfvrTnxZ7aSWhp6cn6+zszDo7O7OIyBYvXpx1dnZmL730UpZlWXbrrbdmuVwue+SRR7LNmzdnV199dTZ69Ogsn88XeeXF8Rd/8RdZLpfLnnzyyWzHjh2F25tvvlmYY89+ad68edmPf/zjbNu2bdmzzz6b3XTTTdlRRx2VrV69Ossye3UofvVTwFlmz37dX/7lX2ZPPvlk9uKLL2br1q3LLrnkkmz48OGFv+PtV19PP/10Vl5enn3729/OXnjhhez+++/Phg0blv3gBz8ozLFn740ALEF33nlnNnbs2Gzo0KHZ2WefXfjKDrLsiSeeyCKi3+1LX/pSlmXvfiXAzTffnFVXV2cVFRXZpz/96Wzz5s3FXXQRDbRXEZH9/d//fWGOPfulP/3TPy38u/fxj388++xnP1uIvyyzV4fi1wPQnvU1ffr0bPTo0dmQIUOympqa7POf/3y2ZcuWwuP2q79/+Zd/ySZMmJBVVFRkp59+enbPPff0edyevTdlWZZlxbn2CABAMfgdQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxPw/ogJKHExBq8sAAAAASUVORK5CYII=",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=640.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 1\n",
    "# Unpack the batch (assuming your dataset returns (inputs, targets))\n",
    "inputs, targets = batch\n",
    "input, target = inputs[k], targets[k]\n",
    "\n",
    "print(\"Inputs shape:\", input.shape)\n",
    "print(\"Targets shape:\", target.shape)\n",
    "\n",
    "output = model(input)\n",
    "\n",
    "print(\"Outputs shape:\", output.shape)\n",
    "target_reshaped = target.reshape(60, 100)\n",
    "target_original = target_reshaped.reshape(6000)\n",
    "target_original\n",
    "plt.close()\n",
    "plt.plot(target, label=\"Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-06 19:12:12,255] A new study created in memory with name: no-name-7d42ee83-9fce-41ff-9fab-897202957042\n",
      "[I 2025-02-06 19:14:22,773] Trial 0 finished with value: 1.4831497163893161 and parameters: {'hidden_size': 68, 'dropout': 0.2796572278236362, 'learning_rate': 0.004450825657499489, 'weight_decay': 0.00027124720064343276}. Best is trial 0 with value: 1.4831497163893161.\n",
      "[I 2025-02-06 19:27:39,277] Trial 1 finished with value: 1.6296354915066404 and parameters: {'hidden_size': 364, 'dropout': 0.49295518977565567, 'learning_rate': 0.006043435602079738, 'weight_decay': 0.007852530176627527}. Best is trial 0 with value: 1.4831497163893161.\n",
      "[I 2025-02-06 19:41:36,139] Trial 2 finished with value: 1.2608569962645957 and parameters: {'hidden_size': 365, 'dropout': 0.4004909576509085, 'learning_rate': 0.0006279622666646788, 'weight_decay': 4.0703178450966685e-06}. Best is trial 2 with value: 1.2608569962645957.\n",
      "[I 2025-02-06 19:49:44,332] Trial 3 finished with value: 1.377703879061068 and parameters: {'hidden_size': 239, 'dropout': 0.25523196879007115, 'learning_rate': 0.0006703771045299565, 'weight_decay': 0.005099303717934173}. Best is trial 2 with value: 1.2608569962645957.\n",
      "[I 2025-02-06 19:59:50,084] Trial 4 finished with value: 1.1867264117939336 and parameters: {'hidden_size': 291, 'dropout': 0.19364281664263827, 'learning_rate': 0.0006865318950616603, 'weight_decay': 1.5849743422819565e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 20:14:45,558] Trial 5 finished with value: 1.5464739893741448 and parameters: {'hidden_size': 383, 'dropout': 0.35655891667507134, 'learning_rate': 0.0010791654331135444, 'weight_decay': 5.632642949339343e-05}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 20:31:39,532] Trial 6 finished with value: 1.4043266576548616 and parameters: {'hidden_size': 424, 'dropout': 0.29520839882397754, 'learning_rate': 0.007300116681659483, 'weight_decay': 4.820065222672352e-05}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 20:39:03,536] Trial 7 finished with value: 1.4162979017563524 and parameters: {'hidden_size': 234, 'dropout': 0.46828539530573987, 'learning_rate': 0.000471375248076495, 'weight_decay': 2.0284562509155787e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 20:43:04,474] Trial 8 finished with value: 1.3916922780989551 and parameters: {'hidden_size': 131, 'dropout': 0.21130225823390503, 'learning_rate': 0.000350828989639619, 'weight_decay': 1.3582661220648346e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 20:57:53,224] Trial 9 finished with value: 1.9863161643074811 and parameters: {'hidden_size': 388, 'dropout': 0.4444613271627782, 'learning_rate': 0.0010716334859724543, 'weight_decay': 2.531461240576309e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 21:18:34,304] Trial 10 finished with value: 1.529993732409863 and parameters: {'hidden_size': 489, 'dropout': 0.11049420527530682, 'learning_rate': 0.00011266595362344224, 'weight_decay': 1.193351584073423e-05}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 21:29:40,660] Trial 11 finished with value: 1.5377962602738753 and parameters: {'hidden_size': 302, 'dropout': 0.39013183529719253, 'learning_rate': 0.0027042732533020235, 'weight_decay': 8.40764641904512e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 21:40:25,698] Trial 12 finished with value: 1.1958037031882163 and parameters: {'hidden_size': 305, 'dropout': 0.17967831202687243, 'learning_rate': 0.00032028752144588294, 'weight_decay': 9.51476883274433e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 21:50:19,079] Trial 13 finished with value: 1.4743621980808923 and parameters: {'hidden_size': 289, 'dropout': 0.16022997818880944, 'learning_rate': 0.0001975179404995467, 'weight_decay': 1.7194929239216046e-05}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 21:56:34,232] Trial 14 finished with value: 1.1908392258413893 and parameters: {'hidden_size': 202, 'dropout': 0.19540239888523192, 'learning_rate': 0.0020725605545255134, 'weight_decay': 1.183548786399151e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 22:02:09,850] Trial 15 finished with value: 2.6620272131938667 and parameters: {'hidden_size': 178, 'dropout': 0.10459915553775946, 'learning_rate': 0.002079839100612295, 'weight_decay': 0.0007903261945980043}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 22:08:24,199] Trial 16 finished with value: 1.314391192587628 and parameters: {'hidden_size': 202, 'dropout': 0.23149704771438512, 'learning_rate': 0.0017466164994190212, 'weight_decay': 1.0082056428120592e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 22:12:32,849] Trial 17 finished with value: 1.4090086134088038 and parameters: {'hidden_size': 134, 'dropout': 0.1664708024501188, 'learning_rate': 0.0015166432904652127, 'weight_decay': 0.0004097597924542043}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 22:20:47,639] Trial 18 finished with value: 1.6273238800042666 and parameters: {'hidden_size': 245, 'dropout': 0.33362628620588053, 'learning_rate': 0.002740754383402328, 'weight_decay': 3.0497066268778455e-05}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 22:25:21,458] Trial 19 finished with value: 1.5643321734838433 and parameters: {'hidden_size': 147, 'dropout': 0.19898091761926945, 'learning_rate': 0.004020581737269553, 'weight_decay': 4.967307145055633e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 22:37:02,847] Trial 20 finished with value: 1.3680042901465406 and parameters: {'hidden_size': 327, 'dropout': 0.14121203979528804, 'learning_rate': 0.00019552987284017213, 'weight_decay': 0.0023975786578493323}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 22:45:53,470] Trial 21 finished with value: 1.5243521167848115 and parameters: {'hidden_size': 264, 'dropout': 0.19135096425878514, 'learning_rate': 0.0003027092096832405, 'weight_decay': 5.036806914431397e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 22:57:35,292] Trial 22 finished with value: 1.5703314289799801 and parameters: {'hidden_size': 329, 'dropout': 0.24576120326944453, 'learning_rate': 0.000764081514388741, 'weight_decay': 2.3474232023620535e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 23:03:42,761] Trial 23 finished with value: 1.369413381209613 and parameters: {'hidden_size': 195, 'dropout': 0.14260601278802285, 'learning_rate': 0.0004085348926440025, 'weight_decay': 1.5759138930149353e-05}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 23:13:07,206] Trial 24 finished with value: 1.361975845324496 and parameters: {'hidden_size': 278, 'dropout': 0.18600871494103438, 'learning_rate': 0.00021988846989253168, 'weight_decay': 1.0468020471234891e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 23:30:57,962] Trial 25 finished with value: 1.439107374645939 and parameters: {'hidden_size': 440, 'dropout': 0.2684794695979295, 'learning_rate': 0.00010378885108671855, 'weight_decay': 6.489923601041209e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 23:42:32,997] Trial 26 finished with value: 1.3793749774410775 and parameters: {'hidden_size': 330, 'dropout': 0.22174650879093105, 'learning_rate': 0.0013756800666160363, 'weight_decay': 2.8854777607586505e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 23:45:08,480] Trial 27 finished with value: 1.3442471728761982 and parameters: {'hidden_size': 88, 'dropout': 0.31833357949709085, 'learning_rate': 0.0006114630422983282, 'weight_decay': 0.00011840208200263025}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 23:52:32,323] Trial 28 finished with value: 2.021361284361896 and parameters: {'hidden_size': 233, 'dropout': 0.13005618991433382, 'learning_rate': 0.0009251902828206253, 'weight_decay': 2.7350667241296795e-05}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 23:54:55,797] Trial 29 finished with value: 1.3747405245166 and parameters: {'hidden_size': 75, 'dropout': 0.16575218713031473, 'learning_rate': 0.00028687650228158767, 'weight_decay': 1.946680281877131e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-06 23:59:53,495] Trial 30 finished with value: 1.421878374541149 and parameters: {'hidden_size': 169, 'dropout': 0.2890679755560639, 'learning_rate': 0.0032986929509432376, 'weight_decay': 0.00011825709947222332}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-07 00:13:20,467] Trial 31 finished with value: 1.3190291822867926 and parameters: {'hidden_size': 360, 'dropout': 0.3954955521195662, 'learning_rate': 0.0005248569721903933, 'weight_decay': 4.183907582635691e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-07 00:24:00,245] Trial 32 finished with value: 1.2703883833166971 and parameters: {'hidden_size': 310, 'dropout': 0.2330557200395165, 'learning_rate': 0.0008151687050746021, 'weight_decay': 8.850129185980588e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-07 00:40:43,943] Trial 33 finished with value: 1.4980719494531305 and parameters: {'hidden_size': 423, 'dropout': 0.42615643576458045, 'learning_rate': 0.005386392209217655, 'weight_decay': 3.465636411965794e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-07 00:53:58,099] Trial 34 finished with value: 1.2230548169582904 and parameters: {'hidden_size': 363, 'dropout': 0.3484978358927232, 'learning_rate': 0.0005947327829179757, 'weight_decay': 1.631361815308677e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-07 01:07:07,741] Trial 35 finished with value: 1.3612183806929468 and parameters: {'hidden_size': 362, 'dropout': 0.35274076232536977, 'learning_rate': 0.0012215275512285175, 'weight_decay': 1.5353185137594975e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-07 01:15:40,441] Trial 36 finished with value: 1.674701767355472 and parameters: {'hidden_size': 262, 'dropout': 0.2689722992911363, 'learning_rate': 0.0005905254847199388, 'weight_decay': 1.029057863621153e-06}. Best is trial 4 with value: 1.1867264117939336.\n",
      "[I 2025-02-07 01:30:51,832] Trial 37 finished with value: 1.143294748031732 and parameters: {'hidden_size': 401, 'dropout': 0.31159803520368773, 'learning_rate': 0.0004282581786099833, 'weight_decay': 3.0769712937166838e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 02:10:40,598] Trial 38 finished with value: 1.3947310707237044 and parameters: {'hidden_size': 504, 'dropout': 0.31242338218575777, 'learning_rate': 0.009306346334746435, 'weight_decay': 3.358049173318259e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 02:29:11,755] Trial 39 finished with value: 1.3920473868737293 and parameters: {'hidden_size': 453, 'dropout': 0.2583776435124376, 'learning_rate': 0.0002505860996226516, 'weight_decay': 7.002029634047581e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 02:36:00,821] Trial 40 finished with value: 1.4137555239971074 and parameters: {'hidden_size': 218, 'dropout': 0.21367530512133023, 'learning_rate': 0.0001427106108823811, 'weight_decay': 7.37559975536793e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 02:49:16,946] Trial 41 finished with value: 1.261903355422728 and parameters: {'hidden_size': 349, 'dropout': 0.3225831104140998, 'learning_rate': 0.00041586759967466176, 'weight_decay': 1.9255567115059455e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 03:03:59,401] Trial 42 finished with value: 1.3187062183802933 and parameters: {'hidden_size': 392, 'dropout': 0.36465954265380257, 'learning_rate': 0.00037121332090870885, 'weight_decay': 1.4870008495012522e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 03:19:20,458] Trial 43 finished with value: 1.3289466678076394 and parameters: {'hidden_size': 402, 'dropout': 0.3658789086881166, 'learning_rate': 0.0006957000046275352, 'weight_decay': 2.6521369751136786e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 03:29:10,012] Trial 44 finished with value: 1.321812079639719 and parameters: {'hidden_size': 296, 'dropout': 0.29273376188803335, 'learning_rate': 0.00046776192919060973, 'weight_decay': 1.1682795339198051e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 03:43:07,841] Trial 45 finished with value: 1.3567763275603104 and parameters: {'hidden_size': 375, 'dropout': 0.337402971476798, 'learning_rate': 0.0020772807591206144, 'weight_decay': 1.5445614624735244e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 04:03:11,278] Trial 46 finished with value: 2.8648619529281962 and parameters: {'hidden_size': 473, 'dropout': 0.18259655907079123, 'learning_rate': 0.0009763869608333691, 'weight_decay': 4.586976992176073e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 04:18:45,597] Trial 47 finished with value: 1.2158096700830983 and parameters: {'hidden_size': 404, 'dropout': 0.48242513216182703, 'learning_rate': 0.0003052938593739495, 'weight_decay': 2.166093795510628e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 04:28:09,551] Trial 48 finished with value: 1.1502202582803287 and parameters: {'hidden_size': 274, 'dropout': 0.4945340206702563, 'learning_rate': 0.0003079644372180521, 'weight_decay': 2.201519817682633e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 04:36:49,830] Trial 49 finished with value: 1.2319097618773935 and parameters: {'hidden_size': 265, 'dropout': 0.12295355744311892, 'learning_rate': 0.000490180711722974, 'weight_decay': 1.2136144612825782e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 04:43:30,980] Trial 50 finished with value: 1.380485304281047 and parameters: {'hidden_size': 217, 'dropout': 0.441677797307589, 'learning_rate': 0.00015219029318168988, 'weight_decay': 4.902867494774783e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 04:59:39,661] Trial 51 finished with value: 1.7510233364657815 and parameters: {'hidden_size': 418, 'dropout': 0.49166908881276383, 'learning_rate': 0.0003169360339278325, 'weight_decay': 2.4680278305992622e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 05:10:33,841] Trial 52 finished with value: 1.3960905238655288 and parameters: {'hidden_size': 309, 'dropout': 0.4866347055191302, 'learning_rate': 0.0002622905398293453, 'weight_decay': 2.0923532619115224e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 05:22:38,006] Trial 53 finished with value: 1.6777779815500564 and parameters: {'hidden_size': 338, 'dropout': 0.45409433389287956, 'learning_rate': 0.00016381515944061264, 'weight_decay': 3.199161609948116e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 05:30:25,963] Trial 54 finished with value: 1.3314066529314943 and parameters: {'hidden_size': 241, 'dropout': 0.41807421625982044, 'learning_rate': 0.00022119536868029727, 'weight_decay': 6.2271826994985275e-06}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 05:40:01,968] Trial 55 finished with value: 1.3223265847053625 and parameters: {'hidden_size': 283, 'dropout': 0.4667966970923503, 'learning_rate': 0.0003540746339929561, 'weight_decay': 0.00020538614756887265}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 05:43:25,085] Trial 56 finished with value: 1.3303418512779486 and parameters: {'hidden_size': 111, 'dropout': 0.20537376397545049, 'learning_rate': 0.00039089650305699976, 'weight_decay': 1.0033974343019895e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 05:48:40,493] Trial 57 finished with value: 1.455707084802486 and parameters: {'hidden_size': 170, 'dropout': 0.4743946076802414, 'learning_rate': 0.0003349549501872126, 'weight_decay': 1.8005105764306175e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 05:59:58,765] Trial 58 finished with value: 1.4071418239489901 and parameters: {'hidden_size': 317, 'dropout': 0.17826608646791003, 'learning_rate': 0.0001897518291034035, 'weight_decay': 8.4475149435852e-05}. Best is trial 37 with value: 1.143294748031732.\n",
      "[I 2025-02-07 06:19:16,652] Trial 59 finished with value: 2.992252331606172 and parameters: {'hidden_size': 462, 'dropout': 0.499596999849549, 'learning_rate': 0.0004412834745592446, 'weight_decay': 5.5052365377444955e-06}. Best is trial 37 with value: 1.143294748031732.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Validation Loss: 1.1433\n",
      "  Params: {'hidden_size': 401, 'dropout': 0.31159803520368773, 'learning_rate': 0.0004282581786099833, 'weight_decay': 3.0769712937166838e-06}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Your dataset should be defined before running Optuna\n",
    "dataset = ...  # Your dataset should be a PyTorch Dataset object\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# **Step 1: Define K-Fold Cross-Validation**\n",
    "def get_kfold_splits(dataset, k=3):\n",
    "    \"\"\"\n",
    "    Splits dataset into k folds for cross-validation.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    indices = list(range(len(dataset)))\n",
    "    return [(list(train_idx), list(val_idx)) for train_idx, val_idx in kf.split(indices)]\n",
    "\n",
    "# **Step 2: Define Optuna Objective Function**\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter tuning with K-Fold cross-validation.\n",
    "    \"\"\"\n",
    "    # **Suggest Hyperparameters**\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 64, 512)\n",
    "    num_layers = 2\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    # **Cross-Validation Setup**\n",
    "    k = 3  # Number of folds\n",
    "    path = r'C:\\Users\\Max Tost\\Desktop\\Notebooks\\SPC Neural Network Project\\Training_data'\n",
    "    features_list = os.listdir(os.path.join(path, r'features'))\n",
    "    # Loading the dataset for this fold\n",
    "    dataset = IndependentCSVDataset(path, features_list)\n",
    "    splits = get_kfold_splits(dataset, k)\n",
    "    fold_losses = []  # Store loss for each fold\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "        # print(f\"Training Fold {fold+1}/{k}\")\n",
    "        \n",
    "\n",
    "        # **Subset datasets for this fold**\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        feature_min, feature_max = compute_global_minmax(train_subset)\n",
    "\n",
    "        global_transform = GlobalMinMaxNormalize(feature_min, feature_max)\n",
    "        train_subset.dataset.transform = global_transform\n",
    "        val_subset.dataset.transform = global_transform  # Same transform to prevent data leakage\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "        # **Create LSTM Model**\n",
    "        model = LSTMModel(input_size=600, hidden_size=hidden_size, num_layers=num_layers, output_size=100, dropout=dropout)\n",
    "        model.to(device)\n",
    "\n",
    "        # **Loss & Optimizer**\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=compute_class_weights(train_loader))\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "        )\n",
    "\n",
    "        # **Training Loop**\n",
    "        num_epochs = 10\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets.float())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # **Validation Loss for Fold**\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                outputs = model(inputs).squeeze()\n",
    "                loss = criterion(outputs, targets.float())\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                total_samples += inputs.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / total_samples\n",
    "        fold_losses.append(avg_val_loss)\n",
    "\n",
    "    # **Return the mean validation loss across all folds**\n",
    "    mean_loss = sum(fold_losses) / len(fold_losses)\n",
    "    return mean_loss\n",
    "\n",
    "# **Step 3: Run Optuna Optimization**\n",
    "study = optuna.create_study(direction=\"minimize\")  # Minimize validation loss\n",
    "study.optimize(objective, n_trials=60)\n",
    "\n",
    "# **Step 4: Best Hyperparameters**\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Validation Loss: {study.best_trial.value:.4f}\")\n",
    "print(\"  Params:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian optimization\n",
    "import BayesianOptimization as bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 8.1832\n",
      "F1 Score: 0.0170\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate_test_metrics(model, test_loader, criterion, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluates the test loss, F1-score, and ROC-AUC of a trained model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained PyTorch model.\n",
    "    - test_loader: DataLoader for the test dataset.\n",
    "    - criterion: Loss function (e.g., nn.BCEWithLogitsLoss).\n",
    "    - device: The device ('cuda' or 'cpu').\n",
    "    - threshold: Decision threshold for binary classification (default=0.5).\n",
    "\n",
    "    Returns:\n",
    "    - Average test loss\n",
    "    - F1-score\n",
    "    - ROC-AUC score\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    total_samples = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to device\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)  # (batch_size, 6000) -> raw logits\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)  # BCEWithLogitsLoss expects raw logits\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            # Convert logits to probabilities using Sigmoid\n",
    "            probs = torch.sigmoid(outputs)  # (batch_size, 6000) -> probabilities in [0,1]\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            preds = (probs > threshold).int()  # Convert to 0 or 1\n",
    "\n",
    "            # Flatten for metric calculations\n",
    "            all_targets.extend(targets.cpu().numpy().flatten())  # Convert to 1D list\n",
    "            all_predictions.extend(preds.cpu().numpy().flatten())  # Convert to 1D list\n",
    "\n",
    "    # Compute average loss\n",
    "    avg_test_loss = test_loss / total_samples\n",
    "\n",
    "    # Compute F1-score\n",
    "    f1 = f1_score(all_targets, all_predictions, average='macro')\n",
    "\n",
    "    return avg_test_loss, f1\n",
    "\n",
    "# Example Usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_loss, test_f1 = evaluate_test_metrics(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
